import os
import re
import sys
import uuid
import time
import json
import asyncio
import aiohttp
import threading
import zipfile
import io
from flask import Flask, render_template, request, redirect, url_for, flash, jsonify, send_file
import requests
from werkzeug.utils import secure_filename

app = Flask(__name__)
app.config['UPLOAD_FOLDER'] = 'uploads'
app.config['ALLOWED_EXTENSIONS'] = {'md'}
app.secret_key = 'super-secret-key'  # ç”Ÿäº§ç¯å¢ƒè¯·æ›¿æ¢ä¸ºéšæœºå­—ç¬¦ä¸²

# ç¡®ä¿ç›®å½•å­˜åœ¨
os.makedirs(app.config['UPLOAD_FOLDER'], exist_ok=True)

# å­˜å‚¨æ‰¹é‡å¤„ç†çŠ¶æ€
batch_status = {}

def allowed_file(filename):
    return '.' in filename and \
           filename.rsplit('.', 1)[1].lower() in app.config['ALLOWED_EXTENSIONS']

def safe_filename(filename):
    """å®‰å…¨çš„æ–‡ä»¶åå¤„ç†ï¼Œæ”¯æŒä¸­æ–‡å­—ç¬¦"""
    import re
    import unicodedata
    
    # å¦‚æœsecure_filenameèƒ½æ­£ç¡®å¤„ç†ï¼Œç›´æ¥ä½¿ç”¨
    secure_name = secure_filename(filename)
    if secure_name and secure_name != filename:
        # å¦‚æœsecure_filenameæ”¹å˜äº†æ–‡ä»¶åï¼Œè¯´æ˜åŸæ–‡ä»¶åæœ‰é—®é¢˜
        # ä½†æˆ‘ä»¬éœ€è¦ä¿ç•™ä¸­æ–‡å­—ç¬¦ï¼Œæ‰€ä»¥ä½¿ç”¨è‡ªå®šä¹‰å¤„ç†
        pass
    else:
        # secure_filenameæ²¡æœ‰æ”¹å˜æ–‡ä»¶åï¼Œè¯´æ˜æ–‡ä»¶åæ˜¯å®‰å…¨çš„
        return filename
    
    # è‡ªå®šä¹‰å¤„ç†ï¼šä¿ç•™ä¸­æ–‡å­—ç¬¦å’ŒåŸºæœ¬ASCIIå­—ç¬¦
    # ç§»é™¤æˆ–æ›¿æ¢å±é™©å­—ç¬¦ï¼Œä½†ä¿ç•™ä¸­æ–‡
    safe_chars = re.sub(r'[<>:"/\\|?*\x00-\x1f]', '_', filename)
    
    # ç§»é™¤å‰åç©ºæ ¼å’Œç‚¹
    safe_chars = safe_chars.strip('. ')
    
    # é™åˆ¶é•¿åº¦
    if len(safe_chars) > 100:
        safe_chars = safe_chars[:100]
    
    # ç¡®ä¿ä¸ä¸ºç©º
    if not safe_chars:
        safe_chars = "file"
    
    return safe_chars

def generate_batch_directory(custom_name=None):
    """ç”Ÿæˆæ‰¹é‡å¤„ç†ç›®å½•å"""
    if custom_name and custom_name.strip():
        # ä½¿ç”¨è‡ªå®šä¹‰åç§°
        clean_name = clean_directory_name(custom_name.strip())
        return clean_name
    else:
        # ä½¿ç”¨éšæœºåç§°
        timestamp = int(time.time())
        random_id = str(uuid.uuid4())[:8]
        return f"batch_{timestamp}_{random_id}"

def clean_directory_name(name):
    """æ¸…ç†ç›®å½•åç§°ï¼Œç§»é™¤éæ³•å­—ç¬¦"""
    import re
    # ç§»é™¤æˆ–æ›¿æ¢éæ³•å­—ç¬¦
    clean_name = re.sub(r'[<>:"/\\|?*\x00-\x1f]', '_', name)
    # ç§»é™¤å‰åç©ºæ ¼å’Œç‚¹
    clean_name = clean_name.strip('. ')
    # é™åˆ¶é•¿åº¦
    if len(clean_name) > 50:
        clean_name = clean_name[:50]
    # ç¡®ä¿ä¸ä¸ºç©º
    if not clean_name:
        clean_name = "custom_batch"
    return clean_name

def clean_text(text, options=None):
    """æ¸…ç†æ–‡æœ¬ï¼Œç§»é™¤ Markdown è¯­æ³•ã€è¡¨æƒ…ç¬¦å·ã€URL é“¾æ¥ç­‰"""
    if not isinstance(text, str):
        text = str(text) if text is not None else ""
    
    cleaned_text = text
    options = options or {}
    
    # ç§»é™¤ Markdown è¯­æ³•
    if options.get('remove_markdown', True):
        # ç§»é™¤å›¾ç‰‡é“¾æ¥
        cleaned_text = re.sub(r'!\[.*?\]\(.*?\)', '', cleaned_text)
        # ç§»é™¤é“¾æ¥ï¼Œä¿ç•™æ–‡æœ¬å†…å®¹
        cleaned_text = re.sub(r'\[([^\]]+)\]\([^)]+\)', r'\1', cleaned_text)
        # ç§»é™¤ç²—ä½“æ ‡è®°
        cleaned_text = re.sub(r'\*\*(.*?)\*\*', r'\1', cleaned_text)
        cleaned_text = re.sub(r'__(.*?)__', r'\1', cleaned_text)
        # ç§»é™¤æ–œä½“æ ‡è®°
        cleaned_text = re.sub(r'\*(.*?)\*', r'\1', cleaned_text)
        cleaned_text = re.sub(r'_(.*?)_', r'\1', cleaned_text)
        # ç§»é™¤ä»£ç å—æ ‡è®°
        cleaned_text = re.sub(r'`([^`]+)`', r'\1', cleaned_text)
        # ç§»é™¤æ ‡é¢˜æ ‡è®°
        cleaned_text = re.sub(r'^#{1,6}\s*', '', cleaned_text, flags=re.MULTILINE)
        # ç§»é™¤åˆ—è¡¨æ ‡è®°
        cleaned_text = re.sub(r'^\s*[-*+]\s*', '', cleaned_text, flags=re.MULTILINE)
        # ç§»é™¤æ•°å­—åˆ—è¡¨æ ‡è®°
        cleaned_text = re.sub(r'^\s*\d+\.\s*', '', cleaned_text, flags=re.MULTILINE)
    
    # ç§»é™¤ URL é“¾æ¥
    if options.get('remove_urls', True):
        cleaned_text = re.sub(r'https?://[^\s]+', '', cleaned_text)
    
    # ç§»é™¤è¡¨æƒ…ç¬¦å·
    if options.get('remove_emoji', True):
        # ä½¿ç”¨æ›´ç²¾ç¡®çš„è¡¨æƒ…ç¬¦å·èŒƒå›´ï¼Œé¿å…è¯¯åˆ ä¸­æ–‡å­—ç¬¦
        emoji_pattern = re.compile(
            "["
            "\U0001F600-\U0001F64F"  # emoticons
            "\U0001F300-\U0001F5FF"  # symbols & pictographs
            "\U0001F680-\U0001F6FF"  # transport & map symbols
            "\U0001F1E0-\U0001F1FF"  # flags (iOS)
            "\U00002702-\U000027B0"
            "\U000024C2-\U0001F251"
            "\U0001F900-\U0001F9FF"  # supplemental symbols
            "\U0001FA70-\U0001FAFF"  # symbols and pictographs extended-a
            "\U00002600-\U000026FF"  # miscellaneous symbols
            "\U00002700-\U000027BF"  # dingbats
            "]+", flags=re.UNICODE)
        cleaned_text = emoji_pattern.sub('', cleaned_text)
    
    # ç§»é™¤å¼•ç”¨æ ‡è®°
    if options.get('remove_citation_numbers', True):
        cleaned_text = re.sub(r'\[\d+\]', '', cleaned_text)
        cleaned_text = re.sub(r'ã€\d+ã€‘', '', cleaned_text)
    
    # åˆå¹¶ä¸ºå•è¡Œæ–‡æœ¬
    if options.get('remove_line_breaks', True):
        # ç§»é™¤æ‰€æœ‰æ¢è¡Œç¬¦
        cleaned_text = re.sub(r'(\r\n|\n|\r)', ' ', cleaned_text)
        # åˆå¹¶å¤šä¸ªè¿ç»­ç©ºæ ¼ä¸ºå•ä¸ªç©ºæ ¼
        cleaned_text = re.sub(r'\s+', ' ', cleaned_text)
    else:
        # åªåˆå¹¶éæ¢è¡Œçš„è¿ç»­ç©ºæ ¼
        cleaned_text = re.sub(r'[ \t]+', ' ', cleaned_text)
    
    return cleaned_text.strip()

async def async_text_to_speech(session, text, output_path, voice="zh-CN-XiaoxiaoNeural", speed=1.0, api_url=None, api_key=None):
    """å¼‚æ­¥è°ƒç”¨TTS APIè½¬æ¢æ–‡æœ¬ä¸ºè¯­éŸ³"""
    # ä½¿ç”¨ä¼ å…¥çš„APIä¿¡æ¯ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤å€¼
    if not api_url:
        api_url = "http://127.0.0.1:5050/v1/audio/speech"
    else:
        # ç¡®ä¿URLæ ¼å¼æ­£ç¡®
        if not api_url.endswith('/v1/audio/speech'):
            api_url = api_url.rstrip('/') + '/v1/audio/speech'
    
    if not api_key:
        api_key = "b77cf8cf852f4080bb56a4adcfc6a685"
    
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    data = {
        "model": "tts-1",
        "input": text,  # ç›´æ¥å‘é€åŸå§‹æ–‡æœ¬ï¼Œè®©APIç«¯å¤„ç†æ¸…ç†
        "voice": voice,
        "speed": speed,
        "cleaning_options": {
            "remove_markdown": True,
            "remove_emoji": True,
            "remove_urls": True,
            "remove_line_breaks": True,
            "remove_citation_numbers": True
        }
    }
    
    try:
        # æ ¹æ®æ–‡æœ¬é•¿åº¦åŠ¨æ€è°ƒæ•´è¶…æ—¶æ—¶é—´
        text_length = len(text)
        if text_length < 10000:  # å°äº1ä¸‡å­—ç¬¦
            timeout_seconds = 300  # 5åˆ†é’Ÿ
        elif text_length < 50000:  # 1-5ä¸‡å­—ç¬¦
            timeout_seconds = 600  # 10åˆ†é’Ÿ
        elif text_length < 100000:  # 5-10ä¸‡å­—ç¬¦
            timeout_seconds = 900  # 15åˆ†é’Ÿ
        else:  # è¶…è¿‡10ä¸‡å­—ç¬¦
            timeout_seconds = 1200  # 20åˆ†é’Ÿ
        
        print(f"ğŸ“ æ–‡æœ¬é•¿åº¦: {text_length:,} å­—ç¬¦ï¼Œè®¾ç½®è¶…æ—¶: {timeout_seconds}ç§’")
        
        # å¼‚æ­¥å‘é€è¯·æ±‚å¹¶è·å–å“åº”
        timeout = aiohttp.ClientTimeout(total=timeout_seconds)
        async with session.post(api_url, headers=headers, json=data, timeout=timeout) as response:
            if response.status == 200:
                # å¼‚æ­¥è¯»å–å“åº”å†…å®¹
                content = await response.read()
                
                # ä¿å­˜éŸ³é¢‘æ–‡ä»¶
                with open(output_path, 'wb') as f:
                    f.write(content)
                
                return True
            else:
                print(f"TTS APIè¿”å›é”™è¯¯çŠ¶æ€ç : {response.status} ({api_url})", file=sys.stderr)
                return False
                
    except asyncio.TimeoutError:
        print(f"â° TTSè½¬æ¢è¶…æ—¶ ({api_url}) - æ–‡æœ¬é•¿åº¦: {len(text):,} å­—ç¬¦", file=sys.stderr)
        return False
    except aiohttp.ClientConnectorError as e:
        print(f"ğŸ”Œ è¿æ¥é”™è¯¯ ({api_url}): {str(e)}", file=sys.stderr)
        return False
    except aiohttp.ClientError as e:
        print(f"ğŸŒ ç½‘ç»œé”™è¯¯ ({api_url}): {str(e)}", file=sys.stderr)
        return False
    except Exception as e:
        print(f"ğŸ’¥ TTSè½¬æ¢å¤±è´¥ ({api_url}): {str(e)}", file=sys.stderr)
        return False

def text_to_speech(text, output_path, voice="zh-CN-XiaoxiaoNeural", speed=1.0, api_url=None, api_key=None):
    """åŒæ­¥ç‰ˆæœ¬çš„TTSè°ƒç”¨ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰"""
    # ä½¿ç”¨ä¼ å…¥çš„APIä¿¡æ¯ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤å€¼
    if not api_url:
        api_url = "http://127.0.0.1:5050/v1/audio/speech"
    else:
        # ç¡®ä¿URLæ ¼å¼æ­£ç¡®
        if not api_url.endswith('/v1/audio/speech'):
            api_url = api_url.rstrip('/') + '/v1/audio/speech'
    
    if not api_key:
        api_key = "b77cf8cf852f4080bb56a4adcfc6a685"
    
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    data = {
        "model": "tts-1",
        "input": text,  # ç›´æ¥å‘é€åŸå§‹æ–‡æœ¬ï¼Œè®©APIç«¯å¤„ç†æ¸…ç†
        "voice": voice,
        "speed": speed,
        "cleaning_options": {
            "remove_markdown": True,
            "remove_emoji": True,
            "remove_urls": True,
            "remove_line_breaks": True,
            "remove_citation_numbers": True
        }
    }
    
    try:
        # å‘é€è¯·æ±‚å¹¶è·å–å“åº”
        response = requests.post(api_url, headers=headers, json=data)
        response.raise_for_status()
        
        # ä¿å­˜éŸ³é¢‘æ–‡ä»¶
        with open(output_path, 'wb') as f:
            f.write(response.content)
        
        return True
    except Exception as e:
        print(f"TTSè½¬æ¢å¤±è´¥ ({api_url}): {str(e)}", file=sys.stderr)
        return False

@app.route('/')
def index():
    return render_template('index.html')

@app.route('/upload', methods=['POST'])
def upload_files():
    if 'files' not in request.files:
        return jsonify({'error': 'æ²¡æœ‰é€‰æ‹©æ–‡ä»¶'}), 400
    
    files = request.files.getlist('files')
    # è·å–å£°éŸ³å’Œè¯­é€Ÿå‚æ•°
    voice = request.form.get('voice', 'zh-CN-XiaoxiaoNeural')
    speed = float(request.form.get('speed', 1.0))
    custom_directory = request.form.get('custom_directory', '').strip()
    
    # è·å–APIæœåŠ¡å™¨ä¿¡æ¯
    api_servers_json = request.form.get('api_servers', '[]')
    concurrency = int(request.form.get('concurrency', 1))
    
    # è§£æAPIæœåŠ¡å™¨åˆ—è¡¨
    try:
        api_servers = json.loads(api_servers_json)
        # è¿‡æ»¤å‡ºå¯ç”¨çš„æœåŠ¡å™¨
        enabled_servers = [server for server in api_servers if server.get('enabled', True)]
        if not enabled_servers:
            return jsonify({'error': 'æ²¡æœ‰å¯ç”¨çš„APIæœåŠ¡å™¨'}), 400
        
        # è°ƒè¯•æ—¥å¿—ï¼šæ˜¾ç¤ºå¯ç”¨çš„æœåŠ¡å™¨
        print(f"ğŸ”§ å¯ç”¨çš„APIæœåŠ¡å™¨åˆ—è¡¨:")
        for i, server in enumerate(enabled_servers):
            print(f"  {i+1}. {server.get('name', 'Unknown')} - {server.get('url', 'No URL')}")
        print(f"ğŸ“Š æ€»å…± {len(enabled_servers)} ä¸ªå¯ç”¨çš„æœåŠ¡å™¨ï¼Œå¹¶å‘åº¦: {concurrency}")
        
    except json.JSONDecodeError:
        return jsonify({'error': 'APIæœåŠ¡å™¨é…ç½®æ ¼å¼é”™è¯¯'}), 400
    
    # ç”Ÿæˆæ‰¹é‡å¤„ç†ç›®å½•
    batch_dir = generate_batch_directory(custom_directory)
    batch_upload_dir = os.path.join(app.config['UPLOAD_FOLDER'], batch_dir)
    
    # åˆ›å»ºæ‰¹é‡å¤„ç†ç›®å½•
    os.makedirs(batch_upload_dir, exist_ok=True)
    
    # åˆå§‹åŒ–æ‰¹é‡çŠ¶æ€
    batch_id = str(uuid.uuid4())
    valid_files = [f for f in files if f and allowed_file(f.filename)]
    batch_status[batch_id] = {
        'total_files': len(valid_files),
        'completed_files': 0,
        'current_file': 0,
        'files': {},
        'server_statuses': {},  # æ·»åŠ æœåŠ¡å™¨çŠ¶æ€è·Ÿè¸ª
        'upload_dir': batch_upload_dir  # ä¿å­˜ä¸Šä¼ ç›®å½•è·¯å¾„
    }
    
    # å…ˆä¿å­˜æ‰€æœ‰æ–‡ä»¶å¹¶åˆå§‹åŒ–çŠ¶æ€
    for file in valid_files:
        filename = safe_filename(file.filename)
        file_id = f"{batch_id}_{filename}"
        
        # ä¿å­˜ä¸Šä¼ çš„Markdownæ–‡ä»¶åˆ°æ‰¹é‡ç›®å½•
        md_path = os.path.join(batch_upload_dir, filename)
        file.save(md_path)
        
        # åˆå§‹åŒ–æ–‡ä»¶çŠ¶æ€
        batch_status[batch_id]['files'][file_id] = {
            'filename': filename,
            'status': 'waiting',
            'progress': 0,
            'stage': 'ç­‰å¾…å¤„ç†'
        }
    
    # å¯åŠ¨åå°å¤„ç†ä»»åŠ¡
    import threading
    thread = threading.Thread(target=run_async_processing, args=(batch_id, batch_upload_dir, voice, speed, enabled_servers, concurrency))
    thread.daemon = True
    thread.start()
    
    return jsonify({
        'batch_id': batch_id,
        'batch_directory': batch_dir,
        'total_files': len(valid_files)
    })

def run_async_processing(batch_id, batch_upload_dir, voice, speed, api_servers, concurrency, specific_files=None):
    """è¿è¡Œå¼‚æ­¥å¤„ç†çš„ä¸»å‡½æ•°"""
    try:
        # åˆ›å»ºæ–°çš„äº‹ä»¶å¾ªç¯
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        
        # è¿è¡Œå¼‚æ­¥å¤„ç†
        loop.run_until_complete(process_files_async(batch_id, batch_upload_dir, voice, speed, api_servers, concurrency, specific_files))
        
    except Exception as e:
        print(f"å¼‚æ­¥å¤„ç†å¼‚å¸¸: {str(e)}", file=sys.stderr)
    finally:
        loop.close()

async def process_files_async(batch_id, batch_upload_dir, voice, speed, api_servers, concurrency, specific_files=None):
    """å¼‚æ­¥å¤„ç†æ–‡ä»¶ï¼Œä½¿ç”¨çœŸæ­£çš„åŠ¨æ€è´Ÿè½½å‡è¡¡"""
    if batch_id not in batch_status:
        return
    
    batch_info = batch_status[batch_id]
    
    # å¦‚æœæŒ‡å®šäº†ç‰¹å®šæ–‡ä»¶ï¼Œåªå¤„ç†è¿™äº›æ–‡ä»¶ï¼›å¦åˆ™å¤„ç†æ‰€æœ‰æ–‡ä»¶
    if specific_files:
        files_to_process = specific_files
        print(f"ğŸ”„ é‡è¯•æ¨¡å¼: åªå¤„ç†æŒ‡å®šçš„ {len(files_to_process)} ä¸ªæ–‡ä»¶")
    else:
        files_to_process = list(batch_info['files'].keys())
        print(f"ğŸ†• å…¨æ–°å¤„ç†: å¤„ç†æ‰€æœ‰ {len(files_to_process)} ä¸ªæ–‡ä»¶")
    
    # è°ƒè¯•æ—¥å¿—ï¼šæ˜¾ç¤ºå¼‚æ­¥å¤„ç†é…ç½®
    print(f"ğŸš€ å¼€å§‹åŠ¨æ€è´Ÿè½½å‡è¡¡å¤„ç†:")
    print(f"  ğŸ“ æ‰¹æ¬¡ID: {batch_id}")
    print(f"  ğŸ“„ æ–‡ä»¶æ•°é‡: {len(files_to_process)}")
    print(f"  ğŸ–¥ï¸ å¯ç”¨æœåŠ¡å™¨: {len(api_servers)}")
    print(f"  âš¡ å¹¶å‘åº¦: {concurrency}")
    
    # åˆ›å»ºaiohttpä¼šè¯ï¼Œä¼˜åŒ–è¿æ¥æ± è®¾ç½®
    connector = aiohttp.TCPConnector(
        limit=concurrency * 3,  # æ€»è¿æ¥æ± é™åˆ¶
        limit_per_host=concurrency * 2,  # æ¯ä¸ªä¸»æœºçš„è¿æ¥é™åˆ¶
        keepalive_timeout=60,  # ä¿æŒè¿æ¥60ç§’
        enable_cleanup_closed=True  # è‡ªåŠ¨æ¸…ç†å…³é—­çš„è¿æ¥
    )
    # ä¼šè¯çº§åˆ«çš„è¶…æ—¶è®¾ç½®æ›´å®½æ¾ï¼Œå› ä¸ºå•ä¸ªè¯·æ±‚çš„è¶…æ—¶ç”±è¯·æ±‚çº§åˆ«æ§åˆ¶
    timeout = aiohttp.ClientTimeout(total=1800)  # 30åˆ†é’Ÿæ€»è¶…æ—¶ï¼ˆä¼šè¯çº§åˆ«ï¼‰
    
    async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:
        # åˆ›å»ºä»»åŠ¡é˜Ÿåˆ—å’ŒæœåŠ¡å™¨çŠ¶æ€è·Ÿè¸ª
        task_queue = asyncio.Queue()
        server_stats = {i: {'active_tasks': 0, 'completed_tasks': 0, 'total_time': 0} for i in range(len(api_servers))}
        
        # åˆå§‹åŒ–æœåŠ¡å™¨çŠ¶æ€
        for i in range(len(api_servers)):
            batch_info['server_statuses'][i] = {
                'name': api_servers[i]['name'],
                'status': 'idle',
                'load': 0,
                'max_load': concurrency,
                'completed_tasks': 0,
                'total_time': 0
            }
        
        # å°†æ‰€æœ‰æ–‡ä»¶æ·»åŠ åˆ°é˜Ÿåˆ—
        for file_id in files_to_process:
            await task_queue.put(file_id)
        
        print(f"ğŸ“¤ åˆ›å»ºä»»åŠ¡é˜Ÿåˆ—: {len(files_to_process)} ä¸ªæ–‡ä»¶")
        
        # åˆ›å»ºçœŸæ­£çš„åŠ¨æ€è´Ÿè½½å‡è¡¡å¤„ç†å™¨
        async def dynamic_load_balancer():
            """çœŸæ­£çš„åŠ¨æ€è´Ÿè½½å‡è¡¡å¤„ç†å™¨ - ä»»åŠ¡å®Œæˆå³åˆ†é…æ–°ä»»åŠ¡ï¼Œå¤±è´¥ä»»åŠ¡è‡ªåŠ¨é‡è¯•"""
            start_time = time.time()
            total_tasks = len(files_to_process)
            completed_tasks = 0
            failed_tasks = []
            retry_queue = asyncio.Queue()  # å¤±è´¥ä»»åŠ¡é‡è¯•é˜Ÿåˆ—
            
            print(f"ğŸš€ å¯åŠ¨åŠ¨æ€è´Ÿè½½å‡è¡¡å™¨:")
            print(f"  ğŸ“Š æ€»ä»»åŠ¡æ•°: {total_tasks}")
            print(f"  ğŸ–¥ï¸ å¯ç”¨æœåŠ¡å™¨: {len(api_servers)}")
            print(f"  âš¡ æ¯æœåŠ¡å™¨å¹¶å‘åº¦: {concurrency}")
            print(f"  ğŸ¯ ç†è®ºæœ€å¤§å¹¶å‘: {len(api_servers) * concurrency}")
            
            # åˆ›å»ºä»»åŠ¡å®Œæˆå›è°ƒå‡½æ•°
            async def on_task_completed(file_id, server_id, success, processing_time):
                nonlocal completed_tasks
                
                # æ›´æ–°æœåŠ¡å™¨ç»Ÿè®¡
                server_stats[server_id]['active_tasks'] -= 1
                server_stats[server_id]['total_time'] += processing_time
                
                # æ›´æ–°æœåŠ¡å™¨çŠ¶æ€
                current_load = server_stats[server_id]['active_tasks']
                batch_info['server_statuses'][server_id]['load'] = current_load
                batch_info['server_statuses'][server_id]['total_time'] = server_stats[server_id]['total_time']
                
                if current_load == 0:
                    batch_info['server_statuses'][server_id]['status'] = 'idle'
                elif current_load >= concurrency:
                    batch_info['server_statuses'][server_id]['status'] = 'full'
                else:
                    batch_info['server_statuses'][server_id]['status'] = 'busy'
                
                # æ£€æŸ¥æ˜¯å¦è¶…æ—¶ï¼ˆè¶…è¿‡5åˆ†é’Ÿè®¤ä¸ºè¶…æ—¶ï¼‰
                if processing_time > 300:
                    print(f"â° ä»»åŠ¡è¶…æ—¶æ£€æµ‹: {file_id} è€—æ—¶ {processing_time:.2f}ç§’ï¼Œå¯èƒ½å­˜åœ¨é—®é¢˜")
                
                if success:
                    server_stats[server_id]['completed_tasks'] += 1
                    batch_info['server_statuses'][server_id]['completed_tasks'] = server_stats[server_id]['completed_tasks']
                    completed_tasks += 1
                    # æ›´æ–°æ‰¹æ¬¡å®Œæˆè®¡æ•°
                    batch_info['completed_files'] = completed_tasks
                    batch_info['current_file'] = completed_tasks
                    print(f"âœ… ä»»åŠ¡å®Œæˆ: {file_id} (æœåŠ¡å™¨: {api_servers[server_id]['name']}, è€—æ—¶: {processing_time:.2f}ç§’)")
                else:
                    # ä»»åŠ¡å¤±è´¥ï¼ŒåŠ å…¥é‡è¯•é˜Ÿåˆ—ï¼ˆåŒ…å«å¤±è´¥æœåŠ¡å™¨ä¿¡æ¯ï¼‰
                    failed_tasks.append(file_id)
                    retry_info = {
                        'file_id': file_id,
                        'failed_server_id': server_id,
                        'failed_server_name': api_servers[server_id]['name']
                    }
                    await retry_queue.put(retry_info)
                    print(f"âŒ ä»»åŠ¡å¤±è´¥: {file_id} (æœåŠ¡å™¨: {api_servers[server_id]['name']}, è€—æ—¶: {processing_time:.2f}ç§’) - åŠ å…¥é‡è¯•é˜Ÿåˆ—")
                
                # ç«‹å³å°è¯•åˆ†é…æ–°ä»»åŠ¡ç»™è¿™ä¸ªæœåŠ¡å™¨
                print(f"ğŸ¯ ä»»åŠ¡å®Œæˆï¼Œæ£€æŸ¥æœåŠ¡å™¨ {api_servers[server_id]['name']} æ˜¯å¦å¯ä»¥æ¥æ”¶æ–°ä»»åŠ¡ (å½“å‰è´Ÿè½½: {current_load}/{concurrency})")
                print(f"ğŸ“Š é˜Ÿåˆ—çŠ¶æ€: ä¸»é˜Ÿåˆ—={task_queue.qsize()}, é‡è¯•é˜Ÿåˆ—={retry_queue.qsize()}, å·²å®Œæˆ={completed_tasks}/{total_tasks}")
                await assign_next_task(server_id)
                
                # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰ä»»åŠ¡éƒ½å·²å®Œæˆ
                if completed_tasks >= total_tasks:
                    print(f"ğŸ‰ æ‰€æœ‰ä»»åŠ¡å·²å®Œæˆï¼")
                    return
            
            # ä»»åŠ¡åˆ†é…å‡½æ•°
            async def assign_next_task(server_id):
                """ä¸ºæŒ‡å®šæœåŠ¡å™¨åˆ†é…ä¸‹ä¸€ä¸ªä»»åŠ¡"""
                current_load = server_stats[server_id]['active_tasks']
                server_name = api_servers[server_id]['name']
                
                print(f"ğŸ” æ£€æŸ¥æœåŠ¡å™¨ {server_name} ä»»åŠ¡åˆ†é… (å½“å‰è´Ÿè½½: {current_load}/{concurrency})")
                
                if current_load >= concurrency:
                    print(f"âš ï¸ æœåŠ¡å™¨ {server_name} å·²æ»¡ï¼Œè·³è¿‡ä»»åŠ¡åˆ†é…")
                    return  # æœåŠ¡å™¨å·²æ»¡
                
                # ä¼˜å…ˆä»é‡è¯•é˜Ÿåˆ—è·å–å¤±è´¥çš„ä»»åŠ¡
                file_id = None
                if not retry_queue.empty():
                    try:
                        retry_info = retry_queue.get_nowait()
                        file_id = retry_info['file_id']
                        failed_server_id = retry_info['failed_server_id']
                        failed_server_name = retry_info['failed_server_name']
                        
                        # æ£€æŸ¥æ˜¯å¦åˆ†é…ç»™ä¸åŒçš„æœåŠ¡å™¨
                        if server_id != failed_server_id:
                            print(f"ğŸ”„ é‡è¯•ä»»åŠ¡: {file_id} â†’ {server_name} (åŸå¤±è´¥æœåŠ¡å™¨: {failed_server_name})")
                        else:
                            # å¦‚æœè¿˜æ˜¯åŒä¸€ä¸ªæœåŠ¡å™¨ï¼Œæ”¾å›é˜Ÿåˆ—ç­‰å¾…å…¶ä»–æœåŠ¡å™¨
                            await retry_queue.put(retry_info)
                            file_id = None
                            print(f"âš ï¸ è·³è¿‡é‡è¯•: {file_id} é¿å…åˆ†é…ç»™åŒä¸€å¤±è´¥æœåŠ¡å™¨ {failed_server_name}")
                    except asyncio.QueueEmpty:
                        pass
                
                # å¦‚æœé‡è¯•é˜Ÿåˆ—ä¸ºç©ºï¼Œä»ä¸»é˜Ÿåˆ—è·å–
                if file_id is None and not task_queue.empty():
                    try:
                        file_id = task_queue.get_nowait()
                        print(f"ğŸ“¤ æ–°ä»»åŠ¡: {file_id} â†’ {server_name}")
                    except asyncio.QueueEmpty:
                        print(f"ğŸ“­ æœåŠ¡å™¨ {server_name} æ— ä»»åŠ¡å¯åˆ†é…")
                        return
                
                if file_id is not None:
                    # æ›´æ–°æœåŠ¡å™¨çŠ¶æ€
                    server_stats[server_id]['active_tasks'] += 1
                    current_load = server_stats[server_id]['active_tasks']
                    batch_info['server_statuses'][server_id]['load'] = current_load
                    
                    if current_load >= concurrency:
                        batch_info['server_statuses'][server_id]['status'] = 'full'
                    else:
                        batch_info['server_statuses'][server_id]['status'] = 'busy'
                    
                    server_name = api_servers[server_id]['name']
                    print(f"ğŸ¯ åŠ¨æ€åˆ†é…: {file_id} â†’ {server_name} (è´Ÿè½½:{current_load}/{concurrency})")
                    print(f"âœ… ä»»åŠ¡åˆ†é…ç¡®è®¤: {file_id} å·²æˆåŠŸåˆ†é…ç»™ {server_name}")
                    
                    # å¯åŠ¨ä»»åŠ¡
                    task = asyncio.create_task(
                        process_single_file_with_callback(
                            session, batch_id, batch_upload_dir, voice, speed, 
                            api_servers, file_id, server_id, server_stats, concurrency,
                            on_task_completed
                        )
                    )
                else:
                    # æ²¡æœ‰ä»»åŠ¡å¯åˆ†é…
                    print(f"ğŸ“­ æœåŠ¡å™¨ {server_name} æ— ä»»åŠ¡å¯åˆ†é… (é‡è¯•é˜Ÿåˆ—: {retry_queue.qsize()}, ä¸»é˜Ÿåˆ—: {task_queue.qsize()})")
                    
                    # å¦‚æœæ‰€æœ‰é˜Ÿåˆ—éƒ½ä¸ºç©ºï¼Œæ£€æŸ¥æ˜¯å¦å¯ä»¥é€€å‡º
                    if task_queue.empty() and retry_queue.empty():
                        active_tasks = sum(server_stats[i]['active_tasks'] for i in range(len(api_servers)))
                        if active_tasks == 0:
                            print(f"ğŸ¯ æ‰€æœ‰ä»»åŠ¡å·²å®Œæˆï¼Œæ‰€æœ‰æœåŠ¡å™¨ç©ºé—²")
                            return
            
            # åˆå§‹åˆ†é…ï¼šä¸ºæ‰€æœ‰æœåŠ¡å™¨åˆ†é…åˆå§‹ä»»åŠ¡
            print(f"ğŸš€ å¼€å§‹åˆå§‹ä»»åŠ¡åˆ†é…...")
            for server_id in range(len(api_servers)):
                for _ in range(min(concurrency, total_tasks)):
                    await assign_next_task(server_id)
                    if task_queue.empty():
                        break
                print(f"  ğŸ“¤ æœåŠ¡å™¨ {api_servers[server_id]['name']} åˆå§‹åˆ†é…å®Œæˆï¼Œå½“å‰è´Ÿè½½: {server_stats[server_id]['active_tasks']}")
            
            print(f"ğŸ“Š åˆå§‹åˆ†é…å®Œæˆï¼Œå‰©ä½™é˜Ÿåˆ—ä»»åŠ¡: {task_queue.qsize()}")
            
            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼ˆäº‹ä»¶é©±åŠ¨ï¼Œæ— éœ€è½®è¯¢ï¼‰
            print(f"ğŸ¯ å¯åŠ¨äº‹ä»¶é©±åŠ¨è´Ÿè½½å‡è¡¡ï¼Œç­‰å¾…ä»»åŠ¡å®Œæˆ...")
            
            # åˆ›å»ºä»»åŠ¡å®Œæˆç­‰å¾…å™¨
            async def wait_for_completion():
                while completed_tasks < total_tasks:
                    # æ£€æŸ¥æ˜¯å¦æœ‰æ´»è·ƒä»»åŠ¡
                    active_tasks = sum(server_stats[i]['active_tasks'] for i in range(len(api_servers)))
                    if active_tasks == 0 and task_queue.empty() and retry_queue.empty():
                        print(f"âš ï¸ æ‰€æœ‰ä»»åŠ¡å·²å®Œæˆä½†è®¡æ•°ä¸åŒ¹é…ï¼Œå¼ºåˆ¶é€€å‡º")
                        break
                    await asyncio.sleep(0.5)  # å‡å°‘æ£€æŸ¥é¢‘ç‡
            
            await wait_for_completion()
            
            total_time = time.time() - start_time
            print(f"ğŸ‰ åŠ¨æ€è´Ÿè½½å‡è¡¡å¤„ç†å®Œæˆ (æ€»è€—æ—¶: {total_time:.2f}ç§’)")
            print(f"ğŸ“Š æœ€ç»ˆç»Ÿè®¡: å®Œæˆ {completed_tasks}/{total_tasks} ä¸ªä»»åŠ¡")
            
            # è¾“å‡ºè¯¦ç»†çš„æœåŠ¡å™¨ç»Ÿè®¡ä¿¡æ¯
            print(f"ğŸ“Š æœåŠ¡å™¨æ€§èƒ½ç»Ÿè®¡:")
            for i, stats in server_stats.items():
                server_name = api_servers[i]['name']
                if stats['completed_tasks'] > 0:
                    avg_time = stats['total_time'] / stats['completed_tasks']
                    throughput = stats['completed_tasks'] / total_time if total_time > 0 else 0
                    print(f"  ğŸ–¥ï¸ {server_name}:")
                    print(f"    âœ… å®Œæˆä»»åŠ¡: {stats['completed_tasks']} ä¸ª")
                    print(f"    â±ï¸ å¹³å‡è€—æ—¶: {avg_time:.2f}ç§’/ä»»åŠ¡")
                    print(f"    ğŸš€ ååé‡: {throughput:.2f}ä»»åŠ¡/ç§’")
                    print(f"    ğŸ“ˆ æ•ˆç‡è¯„åˆ†: {1.0/max(avg_time, 0.1):.2f}")
                else:
                    print(f"  ğŸ–¥ï¸ {server_name}: æœªå¤„ç†ä»»åŠ¡")
        
        # è¿è¡ŒåŠ¨æ€è´Ÿè½½å‡è¡¡å¤„ç†å™¨
        await dynamic_load_balancer()
        
        # ç»Ÿè®¡æœ€ç»ˆç»“æœ
        success_count = sum(1 for file_id in files_to_process 
                           if batch_info['files'][file_id]['status'] == 'completed')
        
        print(f"ğŸ‰ å¼‚æ­¥å¤„ç†å®Œæˆ: {success_count}/{len(files_to_process)} ä¸ªæ–‡ä»¶æˆåŠŸ")
        print(f"ğŸ“Š ä½¿ç”¨äº† {len(api_servers)} ä¸ªæœåŠ¡å™¨ï¼Œå¹¶å‘åº¦: {concurrency}")

async def process_single_file_with_callback(session, batch_id, batch_upload_dir, voice, speed, api_servers, file_id, server_id, server_stats, concurrency, callback):
    """å¼‚æ­¥å¤„ç†å•ä¸ªæ–‡ä»¶ï¼Œå¸¦å›è°ƒæœºåˆ¶"""
    start_time = time.time()
    
    try:
        batch_info = batch_status[batch_id]
        file_info = batch_info['files'][file_id]
        filename = file_info['filename']
        selected_server = api_servers[server_id]
        
        # æ›´æ–°æ–‡ä»¶çŠ¶æ€
        file_info['status'] = 'processing'
        file_info['progress'] = 10
        file_info['stage'] = 'ğŸ“– è¯»å–æ–‡ä»¶...'
        
        # è¯»å–å®Œæ•´çš„Markdownæ–‡ä»¶å†…å®¹
        md_path = os.path.join(batch_upload_dir, filename)
        
        with open(md_path, 'r', encoding='utf-8') as f:
            full_text = f.read()
        
        file_info['progress'] = 20
        file_info['stage'] = 'ğŸ“¤ å‡†å¤‡å‘é€åˆ°TTS API...'
        
        # ç”ŸæˆMP3æ–‡ä»¶è·¯å¾„
        mp3_filename = os.path.splitext(filename)[0] + '.mp3'
        mp3_path = os.path.join(batch_upload_dir, mp3_filename)
        
        file_info['progress'] = 30
        file_info['stage'] = f'ğŸµ æ­£åœ¨è½¬æ¢ (æœåŠ¡å™¨: {selected_server["name"]})...'
        
        # è°ƒç”¨å¼‚æ­¥TTSè½¬æ¢
        api_key = selected_server.get('apiKey', selected_server.get('api_key', ''))
        success = await async_text_to_speech(
            session, full_text, mp3_path, voice, speed, 
            selected_server['url'], api_key
        )
        
        end_time = time.time()
        processing_time = end_time - start_time
        
        if success:
            file_info['status'] = 'completed'
            file_info['progress'] = 100
            file_info['stage'] = 'âœ… è½¬æ¢å®Œæˆ'
            print(f"âœ… {filename} è½¬æ¢æˆåŠŸ (æœåŠ¡å™¨: {selected_server['name']}, è€—æ—¶: {processing_time:.2f}ç§’)")
        else:
            file_info['status'] = 'failed'
            file_info['progress'] = 100
            file_info['stage'] = f'âŒ è½¬æ¢å¤±è´¥ (æœåŠ¡å™¨: {selected_server["name"]})'
            print(f"âŒ {filename} è½¬æ¢å¤±è´¥ (æœåŠ¡å™¨: {selected_server['name']}, è€—æ—¶: {processing_time:.2f}ç§’)")
        
        # è°ƒç”¨å›è°ƒå‡½æ•°
        await callback(file_id, server_id, success, processing_time)
        return success
            
    except Exception as e:
        end_time = time.time()
        processing_time = end_time - start_time
        
        print(f"ğŸ’¥ å¤„ç†æ–‡ä»¶ {file_id} æ—¶å‡ºé”™: {str(e)}", file=sys.stderr)
        if batch_id in batch_status:
            batch_info = batch_status[batch_id]
            if file_id in batch_info['files']:
                file_info = batch_info['files'][file_id]
                file_info['status'] = 'failed'
                file_info['progress'] = 100
                file_info['stage'] = f'ğŸ’¥ å¤„ç†å¼‚å¸¸: {str(e)}'
        
        # è°ƒç”¨å›è°ƒå‡½æ•°
        await callback(file_id, server_id, False, processing_time)
        return False

async def process_single_file_with_server_tracking(session, batch_id, batch_upload_dir, voice, speed, api_servers, file_id, server_id, server_stats, concurrency):
    """å¼‚æ­¥å¤„ç†å•ä¸ªæ–‡ä»¶ï¼Œå¸¦æœåŠ¡å™¨çŠ¶æ€è·Ÿè¸ª"""
    start_time = time.time()
    
    try:
        batch_info = batch_status[batch_id]
        file_info = batch_info['files'][file_id]
        filename = file_info['filename']
        selected_server = api_servers[server_id]
        
        # æ›´æ–°æ–‡ä»¶çŠ¶æ€
        file_info['status'] = 'processing'
        file_info['progress'] = 10
        file_info['stage'] = 'ğŸ“– è¯»å–æ–‡ä»¶...'
        
        # è¯»å–å®Œæ•´çš„Markdownæ–‡ä»¶å†…å®¹
        md_path = os.path.join(batch_upload_dir, filename)
        
        with open(md_path, 'r', encoding='utf-8') as f:
            full_text = f.read()
        
        file_info['progress'] = 20
        file_info['stage'] = 'ğŸ“¤ å‡†å¤‡å‘é€åˆ°TTS API...'
        
        # ç”ŸæˆMP3æ–‡ä»¶è·¯å¾„
        mp3_filename = os.path.splitext(filename)[0] + '.mp3'
        mp3_path = os.path.join(batch_upload_dir, mp3_filename)
        
        file_info['progress'] = 30
        file_info['stage'] = f'ğŸµ æ­£åœ¨è½¬æ¢ (æœåŠ¡å™¨: {selected_server["name"]})...'
        
        # è°ƒç”¨å¼‚æ­¥TTSè½¬æ¢
        api_key = selected_server.get('apiKey', selected_server.get('api_key', ''))
        success = await async_text_to_speech(
            session, full_text, mp3_path, voice, speed, 
            selected_server['url'], api_key
        )
        
        # æ›´æ–°æœåŠ¡å™¨ç»Ÿè®¡
        end_time = time.time()
        processing_time = end_time - start_time
        server_stats[server_id]['active_tasks'] -= 1
        server_stats[server_id]['completed_tasks'] += 1
        server_stats[server_id]['total_time'] += processing_time
        
        # æ›´æ–°æœåŠ¡å™¨çŠ¶æ€
        if batch_id in batch_status:
            batch_info = batch_status[batch_id]
            current_load = server_stats[server_id]['active_tasks']
            batch_info['server_statuses'][server_id]['load'] = current_load
            batch_info['server_statuses'][server_id]['completed_tasks'] = server_stats[server_id]['completed_tasks']
            batch_info['server_statuses'][server_id]['total_time'] = server_stats[server_id]['total_time']
            
            if current_load == 0:
                batch_info['server_statuses'][server_id]['status'] = 'idle'
            elif current_load >= concurrency:
                batch_info['server_statuses'][server_id]['status'] = 'full'
            else:
                batch_info['server_statuses'][server_id]['status'] = 'busy'
        
        if success:
            file_info['status'] = 'completed'
            file_info['progress'] = 100
            file_info['stage'] = 'âœ… è½¬æ¢å®Œæˆ'
            print(f"âœ… {filename} è½¬æ¢æˆåŠŸ (æœåŠ¡å™¨: {selected_server['name']}, è€—æ—¶: {processing_time:.2f}ç§’)")
            return True
        else:
            file_info['status'] = 'failed'
            file_info['progress'] = 100
            file_info['stage'] = f'âŒ è½¬æ¢å¤±è´¥ (æœåŠ¡å™¨: {selected_server["name"]})'
            print(f"âŒ {filename} è½¬æ¢å¤±è´¥ (æœåŠ¡å™¨: {selected_server['name']}, è€—æ—¶: {processing_time:.2f}ç§’)")
            return False
            
    except Exception as e:
        # æ›´æ–°æœåŠ¡å™¨ç»Ÿè®¡
        end_time = time.time()
        processing_time = end_time - start_time
        server_stats[server_id]['active_tasks'] -= 1
        server_stats[server_id]['completed_tasks'] += 1
        server_stats[server_id]['total_time'] += processing_time
        
        # æ›´æ–°æœåŠ¡å™¨çŠ¶æ€
        if batch_id in batch_status:
            batch_info = batch_status[batch_id]
            current_load = server_stats[server_id]['active_tasks']
            batch_info['server_statuses'][server_id]['load'] = current_load
            batch_info['server_statuses'][server_id]['completed_tasks'] = server_stats[server_id]['completed_tasks']
            batch_info['server_statuses'][server_id]['total_time'] = server_stats[server_id]['total_time']
            
            if current_load == 0:
                batch_info['server_statuses'][server_id]['status'] = 'idle'
            elif current_load >= concurrency:
                batch_info['server_statuses'][server_id]['status'] = 'full'
            else:
                batch_info['server_statuses'][server_id]['status'] = 'busy'
        
        print(f"ğŸ’¥ å¤„ç†æ–‡ä»¶ {file_id} æ—¶å‡ºé”™: {str(e)}", file=sys.stderr)
        if batch_id in batch_status:
            batch_info = batch_status[batch_id]
            if file_id in batch_info['files']:
                file_info = batch_info['files'][file_id]
                file_info['status'] = 'failed'
                file_info['progress'] = 100
                file_info['stage'] = f'ğŸ’¥ å¤„ç†å¼‚å¸¸: {str(e)}'
        return False

async def process_single_file_async(session, semaphore, batch_id, batch_upload_dir, voice, speed, api_servers, file_id):
    """å¼‚æ­¥å¤„ç†å•ä¸ªæ–‡ä»¶"""
    async with semaphore:  # æ§åˆ¶å¹¶å‘æ•°
        try:
            batch_info = batch_status[batch_id]
            file_info = batch_info['files'][file_id]
            filename = file_info['filename']
            
            # æ›´æ–°æ–‡ä»¶çŠ¶æ€
            file_info['status'] = 'processing'
            file_info['progress'] = 10
            file_info['stage'] = 'ğŸ“– è¯»å–æ–‡ä»¶...'
            
            # è¯»å–å®Œæ•´çš„Markdownæ–‡ä»¶å†…å®¹
            md_path = os.path.join(batch_upload_dir, filename)
            
            with open(md_path, 'r', encoding='utf-8') as f:
                full_text = f.read()
            
            file_info['progress'] = 20
            file_info['stage'] = 'ğŸ“¤ å‡†å¤‡å‘é€åˆ°TTS API...'
            
            # ç”ŸæˆMP3æ–‡ä»¶è·¯å¾„
            mp3_filename = os.path.splitext(filename)[0] + '.mp3'
            mp3_path = os.path.join(batch_upload_dir, mp3_filename)
            
            file_info['progress'] = 30
            file_info['stage'] = 'â³ ç­‰å¾…TTSå¤„ç†ä¸­...'
            
            # è´Ÿè½½å‡è¡¡ï¼šè½®è¯¢é€‰æ‹©æœåŠ¡å™¨
            server_index = hash(file_id) % len(api_servers)  # ä½¿ç”¨å“ˆå¸Œç¡®ä¿ä¸€è‡´æ€§
            selected_server = api_servers[server_index]
            
            server_name = selected_server.get('name', 'Unknown')
            api_url = selected_server.get('url', '')
            api_key = selected_server.get('apiKey', '')
            
            # è°ƒè¯•æ—¥å¿—ï¼šæ˜¾ç¤ºæœåŠ¡å™¨åˆ†é…
            print(f"ğŸ”„ æ–‡ä»¶ {filename} åˆ†é…ç»™æœåŠ¡å™¨: {server_name} ({api_url})")
            
            file_info['stage'] = f'â³ ä½¿ç”¨æœåŠ¡å™¨ {server_name} å¤„ç†ä¸­...'
            
            # å¼‚æ­¥è°ƒç”¨TTSè½¬æ¢
            success = await async_text_to_speech(session, full_text, mp3_path, voice, speed, api_url, api_key)
            
            if success:
                file_info['progress'] = 90
                file_info['stage'] = 'ğŸ’¾ ä¿å­˜éŸ³é¢‘æ–‡ä»¶...'
                
                file_info['status'] = 'completed'
                file_info['progress'] = 100
                file_info['stage'] = 'âœ… è½¬æ¢å®Œæˆ'
                print(f"âœ… {filename} è½¬æ¢æˆåŠŸ (æœåŠ¡å™¨: {server_name})")
            else:
                file_info['status'] = 'failed'
                file_info['progress'] = 100
                file_info['stage'] = f'âŒ è½¬æ¢å¤±è´¥ (æœåŠ¡å™¨: {server_name})'
                print(f"âŒ {filename} è½¬æ¢å¤±è´¥ (æœåŠ¡å™¨: {server_name})")
            
            # æ›´æ–°å®Œæˆè®¡æ•°
            batch_info['completed_files'] += 1
            batch_info['current_file'] = batch_info['completed_files']
            
            return success
            
        except Exception as e:
            file_info['status'] = 'failed'
            file_info['progress'] = 100
            file_info['stage'] = f'âŒ å¤„ç†å¼‚å¸¸: {str(e)}'
            print(f"âŒ {filename} å¤„ç†å¼‚å¸¸: {str(e)}")
            batch_info['completed_files'] += 1
            batch_info['current_file'] = batch_info['completed_files']
            return False

def process_files_with_load_balancing(batch_id, batch_upload_dir, voice, speed, api_servers, concurrency):
    """ä½¿ç”¨è´Ÿè½½å‡è¡¡å’Œå¹¶å‘å¤„ç†æ–‡ä»¶"""
    if batch_id not in batch_status:
        return
    
    batch_info = batch_status[batch_id]
    files_to_process = list(batch_info['files'].keys())
    
    # è°ƒè¯•æ—¥å¿—ï¼šæ˜¾ç¤ºè´Ÿè½½å‡è¡¡é…ç½®
    print(f"ğŸš€ å¼€å§‹è´Ÿè½½å‡è¡¡å¤„ç†:")
    print(f"  ğŸ“ æ‰¹æ¬¡ID: {batch_id}")
    print(f"  ğŸ“„ æ–‡ä»¶æ•°é‡: {len(files_to_process)}")
    print(f"  ğŸ–¥ï¸ å¯ç”¨æœåŠ¡å™¨: {len(api_servers)}")
    print(f"  âš¡ å¹¶å‘åº¦: {concurrency}")
    
    # åˆ›å»ºæœåŠ¡å™¨è½®è¯¢ç´¢å¼•
    server_index = 0
    
    # ä½¿ç”¨çº¿ç¨‹æ± è¿›è¡Œå¹¶å‘å¤„ç†
    import threading
    from concurrent.futures import ThreadPoolExecutor, as_completed
    import queue
    
    # åˆ›å»ºä»»åŠ¡é˜Ÿåˆ—
    task_queue = queue.Queue()
    for file_id in files_to_process:
        task_queue.put(file_id)
    
    # åˆ›å»ºçº¿ç¨‹æ± 
    max_workers = min(concurrency, len(files_to_process), len(api_servers))
    
    def process_single_file(file_id):
        """å¤„ç†å•ä¸ªæ–‡ä»¶"""
        try:
            file_info = batch_info['files'][file_id]
            filename = file_info['filename']
            
            # æ›´æ–°æ–‡ä»¶çŠ¶æ€
            file_info['status'] = 'processing'
            file_info['progress'] = 10
            file_info['stage'] = 'ğŸ“– è¯»å–æ–‡ä»¶...'
            
            # è¯»å–å®Œæ•´çš„Markdownæ–‡ä»¶å†…å®¹
            md_path = os.path.join(batch_upload_dir, filename)
            
            with open(md_path, 'r', encoding='utf-8') as f:
                full_text = f.read()
            
            file_info['progress'] = 20
            file_info['stage'] = 'ğŸ“¤ å‡†å¤‡å‘é€åˆ°TTS API...'
            
            # ç”ŸæˆMP3æ–‡ä»¶è·¯å¾„
            mp3_filename = os.path.splitext(filename)[0] + '.mp3'
            mp3_path = os.path.join(batch_upload_dir, mp3_filename)
            
            file_info['progress'] = 30
            file_info['stage'] = 'â³ ç­‰å¾…TTSå¤„ç†ä¸­...'
            
            # è´Ÿè½½å‡è¡¡ï¼šè½®è¯¢é€‰æ‹©æœåŠ¡å™¨
            nonlocal server_index
            selected_server = api_servers[server_index % len(api_servers)]
            server_index += 1
            
            server_name = selected_server.get('name', 'Unknown')
            api_url = selected_server.get('url', '')
            api_key = selected_server.get('apiKey', '')
            
            # è°ƒè¯•æ—¥å¿—ï¼šæ˜¾ç¤ºæœåŠ¡å™¨åˆ†é…
            print(f"ğŸ”„ æ–‡ä»¶ {filename} åˆ†é…ç»™æœåŠ¡å™¨: {server_name} ({api_url})")
            
            file_info['stage'] = f'â³ ä½¿ç”¨æœåŠ¡å™¨ {server_name} å¤„ç†ä¸­...'
            
            # è°ƒç”¨TTSè½¬æ¢
            success = text_to_speech(full_text, mp3_path, voice, speed, api_url, api_key)
            
            if success:
                file_info['progress'] = 90
                file_info['stage'] = 'ğŸ’¾ ä¿å­˜éŸ³é¢‘æ–‡ä»¶...'
                
                file_info['status'] = 'completed'
                file_info['progress'] = 100
                file_info['stage'] = 'âœ… è½¬æ¢å®Œæˆ'
                print(f"âœ… {filename} è½¬æ¢æˆåŠŸ (æœåŠ¡å™¨: {server_name})")
            else:
                file_info['status'] = 'failed'
                file_info['progress'] = 100
                file_info['stage'] = f'âŒ è½¬æ¢å¤±è´¥ (æœåŠ¡å™¨: {server_name})'
                print(f"âŒ {filename} è½¬æ¢å¤±è´¥ (æœåŠ¡å™¨: {server_name})")
            
            return file_id, success
            
        except Exception as e:
            file_info['status'] = 'failed'
            file_info['progress'] = 100
            file_info['stage'] = f'âŒ å¤„ç†å¼‚å¸¸: {str(e)}'
            print(f"âŒ {filename} å¤„ç†å¼‚å¸¸: {str(e)}")
            return file_id, False
    
    # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œä»»åŠ¡
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        future_to_file = {
            executor.submit(process_single_file, file_id): file_id 
            for file_id in files_to_process
        }
        
        # å¤„ç†å®Œæˆçš„ä»»åŠ¡
        for future in as_completed(future_to_file):
            file_id, success = future.result()
            batch_info['completed_files'] += 1
            
            # æ›´æ–°å½“å‰å¤„ç†æ–‡ä»¶è®¡æ•°
            batch_info['current_file'] = batch_info['completed_files']
    
    print(f"ğŸ‰ æ‰¹é‡å¤„ç†å®Œæˆ: {batch_info['completed_files']}/{batch_info['total_files']} ä¸ªæ–‡ä»¶")
    print(f"ğŸ“Š ä½¿ç”¨äº† {len(api_servers)} ä¸ªæœåŠ¡å™¨ï¼Œå¹¶å‘åº¦: {max_workers}")

@app.route('/progress/<batch_id>')
def get_progress(batch_id):
    """è·å–æ‰¹é‡å¤„ç†è¿›åº¦"""
    if batch_id not in batch_status:
        return jsonify({'error': 'æ‰¹æ¬¡ä¸å­˜åœ¨'}), 404
    
    status = batch_status[batch_id]
    return jsonify({
        'batch_id': batch_id,
        'total_files': status['total_files'],
        'completed_files': status['completed_files'],
        'current_file': status.get('current_file', 0),
        'files': status['files']
    })

@app.route('/server_status/<batch_id>')
def get_server_status(batch_id):
    """è·å–æœåŠ¡å™¨çŠ¶æ€ä¿¡æ¯"""
    if batch_id not in batch_status:
        return jsonify({'error': 'æ‰¹æ¬¡ä¸å­˜åœ¨'}), 404
    
    # ä»batch_statusä¸­è·å–æœåŠ¡å™¨çŠ¶æ€ä¿¡æ¯
    status = batch_status[batch_id]
    server_statuses = status.get('server_statuses', {})
    
    return jsonify({
        'batch_id': batch_id,
        'server_statuses': server_statuses,
        'timestamp': time.time()
    })

@app.route('/retry_failed', methods=['POST'])
def retry_failed_files():
    """é‡è¯•å¤±è´¥çš„æ–‡ä»¶"""
    try:
        batch_id = request.form.get('batch_id')
        api_servers_json = request.form.get('api_servers')
        concurrency = int(request.form.get('concurrency', 2))
        voice = request.form.get('voice', 'zh-CN-XiaoxiaoNeural')
        speed = float(request.form.get('speed', 1.0))
        
        if not batch_id or batch_id not in batch_status:
            return jsonify({'error': 'æ‰¹æ¬¡ä¸å­˜åœ¨'}), 404
        
        # è§£æAPIæœåŠ¡å™¨åˆ—è¡¨
        try:
            api_servers = json.loads(api_servers_json) if api_servers_json else []
        except json.JSONDecodeError:
            return jsonify({'error': 'APIæœåŠ¡å™¨é…ç½®æ ¼å¼é”™è¯¯'}), 400
        
        # è¿‡æ»¤å¯ç”¨çš„æœåŠ¡å™¨
        enabled_servers = [server for server in api_servers if server.get('enabled', False)]
        if not enabled_servers:
            return jsonify({'error': 'æ²¡æœ‰å¯ç”¨çš„APIæœåŠ¡å™¨'}), 400
        
        batch_info = batch_status[batch_id]
        
        # æ‰¾å‡ºå¤±è´¥çš„æ–‡ä»¶
        failed_files = []
        for file_id, file_info in batch_info['files'].items():
            if file_info['status'] == 'failed':
                failed_files.append(file_id)
        
        if not failed_files:
            return jsonify({'error': 'æ²¡æœ‰å¤±è´¥çš„æ–‡ä»¶éœ€è¦é‡è¯•'}), 400
        
        print(f"ğŸ”„ å¼€å§‹é‡è¯•å¤±è´¥æ–‡ä»¶:")
        print(f"  ğŸ“ æ‰¹æ¬¡ID: {batch_id}")
        print(f"  ğŸ“„ å¤±è´¥æ–‡ä»¶æ•°é‡: {len(failed_files)}")
        print(f"  ğŸ–¥ï¸ å¯ç”¨æœåŠ¡å™¨: {len(enabled_servers)}")
        print(f"  âš¡ å¹¶å‘åº¦: {concurrency}")
        
        # é‡ç½®å¤±è´¥æ–‡ä»¶çš„çŠ¶æ€
        for file_id in failed_files:
            file_info = batch_info['files'][file_id]
            file_info['status'] = 'pending'
            file_info['progress'] = 0
            file_info['stage'] = 'â³ ç­‰å¾…é‡è¯•...'
            file_info['error'] = None
        
        # æ›´æ–°æ‰¹æ¬¡çŠ¶æ€
        batch_info['status'] = 'processing'
        batch_info['completed_files'] = batch_info['total_files'] - len(failed_files)
        batch_info['current_file'] = batch_info['completed_files']
        
        # è·å–æ‰¹æ¬¡ç›®å½•
        batch_upload_dir = batch_info['upload_dir']
        
        # å¯åŠ¨å¼‚æ­¥é‡è¯•å¤„ç†
        retry_thread = threading.Thread(
            target=run_async_processing,
            args=(batch_id, batch_upload_dir, voice, speed, enabled_servers, concurrency, failed_files)
        )
        retry_thread.daemon = True
        retry_thread.start()
        
        return jsonify({
            'success': True,
            'message': f'å¼€å§‹é‡è¯• {len(failed_files)} ä¸ªå¤±è´¥æ–‡ä»¶',
            'retry_files': len(failed_files)
        })
        
    except Exception as e:
        print(f"é‡è¯•å¤±è´¥æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}", file=sys.stderr)
        return jsonify({'error': f'é‡è¯•å¤±è´¥: {str(e)}'}), 500

@app.route('/api/folders')
def get_folders():
    """è·å–uploadsç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶å¤¹åˆ—è¡¨"""
    try:
        upload_dir = app.config['UPLOAD_FOLDER']
        if not os.path.exists(upload_dir):
            return jsonify({'folders': []})
        
        folders = []
        for item in os.listdir(upload_dir):
            item_path = os.path.join(upload_dir, item)
            if os.path.isdir(item_path):
                # è·å–æ–‡ä»¶å¤¹ä¿¡æ¯
                files = os.listdir(item_path)
                md_files = [f for f in files if f.endswith('.md')]
                mp3_files = [f for f in files if f.endswith('.mp3')]
                
                # è·å–æ–‡ä»¶å¤¹åˆ›å»ºæ—¶é—´
                create_time = os.path.getctime(item_path)
                
                folders.append({
                    'name': item,
                    'path': item_path,
                    'md_count': len(md_files),
                    'mp3_count': len(mp3_files),
                    'total_files': len(files),
                    'create_time': time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(create_time))
                })
        
        # æŒ‰åˆ›å»ºæ—¶é—´å€’åºæ’åˆ—
        folders.sort(key=lambda x: x['create_time'], reverse=True)
        return jsonify({'folders': folders})
    
    except Exception as e:
        return jsonify({'error': f'è·å–æ–‡ä»¶å¤¹åˆ—è¡¨å¤±è´¥: {str(e)}'}), 500

@app.route('/api/download/<folder_name>')
def download_folder(folder_name):
    """ä¸‹è½½æŒ‡å®šæ–‡ä»¶å¤¹çš„ZIPåŒ…"""
    try:
        # å®‰å…¨æ£€æŸ¥ï¼šé˜²æ­¢è·¯å¾„éå†æ”»å‡»
        if '..' in folder_name or '/' in folder_name or '\\' in folder_name:
            return jsonify({'error': 'æ— æ•ˆçš„æ–‡ä»¶å¤¹åç§°'}), 400
        
        upload_dir = app.config['UPLOAD_FOLDER']
        folder_path = os.path.join(upload_dir, folder_name)
        
        if not os.path.exists(folder_path) or not os.path.isdir(folder_path):
            return jsonify({'error': 'æ–‡ä»¶å¤¹ä¸å­˜åœ¨'}), 404
        
        # åˆ›å»ºå†…å­˜ä¸­çš„ZIPæ–‡ä»¶
        memory_file = io.BytesIO()
        
        with zipfile.ZipFile(memory_file, 'w', zipfile.ZIP_DEFLATED) as zipf:
            for root, dirs, files in os.walk(folder_path):
                for file in files:
                    file_path = os.path.join(root, file)
                    # è®¡ç®—ç›¸å¯¹è·¯å¾„ï¼Œä¿æŒæ–‡ä»¶å¤¹ç»“æ„
                    arcname = os.path.relpath(file_path, folder_path)
                    zipf.write(file_path, arcname)
        
        memory_file.seek(0)
        
        # ç”Ÿæˆä¸‹è½½æ–‡ä»¶å
        download_filename = f"{folder_name}.zip"
        
        return send_file(
            memory_file,
            as_attachment=True,
            download_name=download_filename,
            mimetype='application/zip'
        )
    
    except Exception as e:
        return jsonify({'error': f'ä¸‹è½½å¤±è´¥: {str(e)}'}), 500

@app.route('/api/delete/<folder_name>', methods=['DELETE'])
def delete_folder(folder_name):
    """åˆ é™¤æŒ‡å®šæ–‡ä»¶å¤¹"""
    try:
        # å®‰å…¨æ£€æŸ¥ï¼šé˜²æ­¢è·¯å¾„éå†æ”»å‡»
        if '..' in folder_name or '/' in folder_name or '\\' in folder_name:
            return jsonify({'error': 'æ— æ•ˆçš„æ–‡ä»¶å¤¹åç§°'}), 400
        
        upload_dir = app.config['UPLOAD_FOLDER']
        folder_path = os.path.join(upload_dir, folder_name)
        
        if not os.path.exists(folder_path) or not os.path.isdir(folder_path):
            return jsonify({'error': 'æ–‡ä»¶å¤¹ä¸å­˜åœ¨'}), 404
        
        # åˆ é™¤æ–‡ä»¶å¤¹åŠå…¶å†…å®¹
        import shutil
        shutil.rmtree(folder_path)
        
        return jsonify({'message': f'æ–‡ä»¶å¤¹ {folder_name} åˆ é™¤æˆåŠŸ'})
    
    except Exception as e:
        return jsonify({'error': f'åˆ é™¤å¤±è´¥: {str(e)}'}), 500

if __name__ == '__main__':
    # æ”¯æŒDockeréƒ¨ç½²ï¼Œç›‘å¬æ‰€æœ‰æ¥å£
    import os
    host = os.environ.get('FLASK_HOST', '0.0.0.0')
    port = int(os.environ.get('FLASK_PORT', 5055))
    debug = os.environ.get('FLASK_ENV', 'development') == 'development'
    
    print(f"ğŸš€ å¯åŠ¨TTSæ‰¹é‡è½¬æ¢æœåŠ¡...")
    print(f"ğŸ“ ç›‘å¬åœ°å€: {host}:{port}")
    print(f"ğŸ”§ è°ƒè¯•æ¨¡å¼: {debug}")
    
    app.run(host=host, port=port, debug=debug)