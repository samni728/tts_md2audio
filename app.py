import os
import re
import sys
import uuid
import time
import json
import asyncio
import aiohttp
import threading
import zipfile
import io
import random
import contextlib
from collections import deque, defaultdict
from flask import Flask, render_template, request, redirect, url_for, flash, jsonify, send_file
import requests
from werkzeug.utils import secure_filename

app = Flask(__name__)
app.config['UPLOAD_FOLDER'] = 'uploads'
app.config['ALLOWED_EXTENSIONS'] = {'md'}
app.secret_key = 'super-secret-key'  # ç”Ÿäº§ç¯å¢ƒè¯·æ›¿æ¢ä¸ºéšæœºå­—ç¬¦ä¸²

# ç¡®ä¿ç›®å½•å­˜åœ¨
os.makedirs(app.config['UPLOAD_FOLDER'], exist_ok=True)

# å­˜å‚¨æ‰¹é‡å¤„ç†çŠ¶æ€
batch_status = {}

# é»˜è®¤æäº¤æ¥å£çš„æ¸…æ´—é…ç½®
DEFAULT_CLEANING_OPTIONS = {
    "remove_markdown": True,
    "remove_emoji": True,
    "remove_urls": True,
    "remove_line_breaks": True,
    "remove_citation_numbers": True
}

# æœ€å°éŸ³é¢‘æœ‰æ•ˆæ€§åˆ¤å®šé…ç½®
MIN_AUDIO_SIZE_BYTES = int(os.environ.get("TTS_MIN_AUDIO_SIZE_BYTES", 4096))
MIN_AUDIO_BYTES_PER_CHAR = float(
    os.environ.get("TTS_MIN_AUDIO_BYTES_PER_CHAR", "3.0")
)

# å…¨å±€APIå¹¶å‘ä¸Šé™ï¼ˆä»…å½“æ˜¾å¼é…ç½® >0 æ—¶å¯ç”¨ï¼›é»˜è®¤ç¦ç”¨ï¼ŒæŒ‰æœåŠ¡å™¨ç‹¬ç«‹å¤„ç†ï¼‰
def _init_global_semaphore():
    value = os.environ.get('GLOBAL_CONCURRENCY_LIMIT', '0')
    try:
        limit = int(value)
    except ValueError:
        limit = 0
    if limit and limit > 0:
        return asyncio.Semaphore(limit)
    return None

global_api_semaphore = _init_global_semaphore()

def allowed_file(filename):
    return '.' in filename and \
           filename.rsplit('.', 1)[1].lower() in app.config['ALLOWED_EXTENSIONS']

def safe_filename(filename):
    """å®‰å…¨çš„æ–‡ä»¶åå¤„ç†ï¼Œæ”¯æŒä¸­æ–‡å­—ç¬¦"""
    import re
    import unicodedata
    
    # å¦‚æœsecure_filenameèƒ½æ­£ç¡®å¤„ç†ï¼Œç›´æ¥ä½¿ç”¨
    secure_name = secure_filename(filename)
    if secure_name and secure_name != filename:
        # å¦‚æœsecure_filenameæ”¹å˜äº†æ–‡ä»¶åï¼Œè¯´æ˜åŸæ–‡ä»¶åæœ‰é—®é¢˜
        # ä½†æˆ‘ä»¬éœ€è¦ä¿ç•™ä¸­æ–‡å­—ç¬¦ï¼Œæ‰€ä»¥ä½¿ç”¨è‡ªå®šä¹‰å¤„ç†
        pass
    else:
        # secure_filenameæ²¡æœ‰æ”¹å˜æ–‡ä»¶åï¼Œè¯´æ˜æ–‡ä»¶åæ˜¯å®‰å…¨çš„
        return filename
    
    # è‡ªå®šä¹‰å¤„ç†ï¼šä¿ç•™ä¸­æ–‡å­—ç¬¦å’ŒåŸºæœ¬ASCIIå­—ç¬¦
    # ç§»é™¤æˆ–æ›¿æ¢å±é™©å­—ç¬¦ï¼Œä½†ä¿ç•™ä¸­æ–‡
    safe_chars = re.sub(r'[<>:"/\\|?*\x00-\x1f]', '_', filename)
    
    # ç§»é™¤å‰åç©ºæ ¼å’Œç‚¹
    safe_chars = safe_chars.strip('. ')
    
    # é™åˆ¶é•¿åº¦
    if len(safe_chars) > 100:
        safe_chars = safe_chars[:100]
    
    # ç¡®ä¿ä¸ä¸ºç©º
    if not safe_chars:
        safe_chars = "file"
    
    return safe_chars

def generate_batch_directory(custom_name=None):
    """ç”Ÿæˆæ‰¹é‡å¤„ç†ç›®å½•å"""
    if custom_name and custom_name.strip():
        # ä½¿ç”¨è‡ªå®šä¹‰åç§°
        clean_name = clean_directory_name(custom_name.strip())
        return clean_name
    else:
        # ä½¿ç”¨éšæœºåç§°
        timestamp = int(time.time())
        random_id = str(uuid.uuid4())[:8]
        return f"batch_{timestamp}_{random_id}"

def clean_directory_name(name):
    """æ¸…ç†ç›®å½•åç§°ï¼Œç§»é™¤éæ³•å­—ç¬¦"""
    import re
    # ç§»é™¤æˆ–æ›¿æ¢éæ³•å­—ç¬¦
    clean_name = re.sub(r'[<>:"/\\|?*\x00-\x1f]', '_', name)
    # ç§»é™¤å‰åç©ºæ ¼å’Œç‚¹
    clean_name = clean_name.strip('. ')
    # é™åˆ¶é•¿åº¦
    if len(clean_name) > 50:
        clean_name = clean_name[:50]
    # ç¡®ä¿ä¸ä¸ºç©º
    if not clean_name:
        clean_name = "custom_batch"
    return clean_name

def clean_text(text, options=None):
    """æ¸…ç†æ–‡æœ¬ï¼Œç§»é™¤ Markdown è¯­æ³•ã€è¡¨æƒ…ç¬¦å·ã€URL é“¾æ¥ç­‰"""
    if not isinstance(text, str):
        text = str(text) if text is not None else ""
    
    cleaned_text = text
    options = options or {}
    
    # ç§»é™¤ Markdown è¯­æ³•
    if options.get('remove_markdown', True):
        # ç§»é™¤å›¾ç‰‡é“¾æ¥
        cleaned_text = re.sub(r'!\[.*?\]\(.*?\)', '', cleaned_text)
        # ç§»é™¤é“¾æ¥ï¼Œä¿ç•™æ–‡æœ¬å†…å®¹
        cleaned_text = re.sub(r'\[([^\]]+)\]\([^)]+\)', r'\1', cleaned_text)
        # ç§»é™¤ç²—ä½“æ ‡è®°
        cleaned_text = re.sub(r'\*\*(.*?)\*\*', r'\1', cleaned_text)
        cleaned_text = re.sub(r'__(.*?)__', r'\1', cleaned_text)
        # ç§»é™¤æ–œä½“æ ‡è®°
        cleaned_text = re.sub(r'\*(.*?)\*', r'\1', cleaned_text)
        cleaned_text = re.sub(r'_(.*?)_', r'\1', cleaned_text)
        # ç§»é™¤ä»£ç å—æ ‡è®°
        cleaned_text = re.sub(r'`([^`]+)`', r'\1', cleaned_text)
        # ç§»é™¤æ ‡é¢˜æ ‡è®°
        cleaned_text = re.sub(r'^#{1,6}\s*', '', cleaned_text, flags=re.MULTILINE)
        # ç§»é™¤åˆ—è¡¨æ ‡è®°
        cleaned_text = re.sub(r'^\s*[-*+]\s*', '', cleaned_text, flags=re.MULTILINE)
        # ç§»é™¤æ•°å­—åˆ—è¡¨æ ‡è®°
        cleaned_text = re.sub(r'^\s*\d+\.\s*', '', cleaned_text, flags=re.MULTILINE)
    
    # ç§»é™¤ URL é“¾æ¥
    if options.get('remove_urls', True):
        cleaned_text = re.sub(r'https?://[^\s]+', '', cleaned_text)
    
    # ç§»é™¤è¡¨æƒ…ç¬¦å·
    if options.get('remove_emoji', True):
        # ä½¿ç”¨æ›´ç²¾ç¡®çš„è¡¨æƒ…ç¬¦å·èŒƒå›´ï¼Œé¿å…è¯¯åˆ ä¸­æ–‡å­—ç¬¦
        emoji_pattern = re.compile(
            "["
            "\U0001F600-\U0001F64F"  # emoticons
            "\U0001F300-\U0001F5FF"  # symbols & pictographs
            "\U0001F680-\U0001F6FF"  # transport & map symbols
            "\U0001F1E0-\U0001F1FF"  # flags (iOS)
            "\U00002702-\U000027B0"
            "\U000024C2-\U0001F251"
            "\U0001F900-\U0001F9FF"  # supplemental symbols
            "\U0001FA70-\U0001FAFF"  # symbols and pictographs extended-a
            "\U00002600-\U000026FF"  # miscellaneous symbols
            "\U00002700-\U000027BF"  # dingbats
            "]+", flags=re.UNICODE)
        cleaned_text = emoji_pattern.sub('', cleaned_text)
    
    # ç§»é™¤å¼•ç”¨æ ‡è®°
    if options.get('remove_citation_numbers', True):
        cleaned_text = re.sub(r'\[\d+\]', '', cleaned_text)
        cleaned_text = re.sub(r'ã€\d+ã€‘', '', cleaned_text)
    
    # åˆå¹¶ä¸ºå•è¡Œæ–‡æœ¬
    if options.get('remove_line_breaks', True):
        # ç§»é™¤æ‰€æœ‰æ¢è¡Œç¬¦
        cleaned_text = re.sub(r'(\r\n|\n|\r)', ' ', cleaned_text)
        # åˆå¹¶å¤šä¸ªè¿ç»­ç©ºæ ¼ä¸ºå•ä¸ªç©ºæ ¼
        cleaned_text = re.sub(r'\s+', ' ', cleaned_text)
    else:
        # åªåˆå¹¶éæ¢è¡Œçš„è¿ç»­ç©ºæ ¼
        cleaned_text = re.sub(r'[ \t]+', ' ', cleaned_text)
    
    return cleaned_text.strip()

async def async_text_to_speech(session, text, output_path, voice="zh-CN-XiaoxiaoNeural", speed=1.0, api_url=None, api_key=None, timeout_seconds: int = 300, pitch: float = 1.0, cleaning_options=None, response_format: str = "mp3"):
    """å¼‚æ­¥è°ƒç”¨TTS APIè½¬æ¢æ–‡æœ¬ä¸ºè¯­éŸ³ï¼ˆå›ºå®šè¶…æ—¶ï¼Œç§»é™¤æŒ‰å­—æ•°åŠ¨æ€è¶…æ—¶ï¼‰ã€‚

    è¿”å› (success, status_code, error_detail) å…ƒç»„ï¼Œä¾¿äºä¸Šå±‚é’ˆå¯¹é™æµ/è¶…æ—¶ç­‰æƒ…å†µåšç²¾ç»†åŒ–å¤„ç†ã€‚
    å½“å‡ºç°ç½‘ç»œå¼‚å¸¸ã€è¶…æ—¶ç­‰æƒ…å†µæ—¶ status_code å¯èƒ½ä¸º Noneï¼ŒåŒæ—¶ error_detail æä¾›ç®€çŸ­è¯´æ˜ã€‚
    """
    # ä½¿ç”¨ä¼ å…¥çš„APIä¿¡æ¯ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤å€¼
    if not api_url:
        api_url = "http://127.0.0.1:5050/v1/audio/speech"
    else:
        # ç¡®ä¿URLæ ¼å¼æ­£ç¡®
        if not api_url.endswith('/v1/audio/speech'):
            api_url = api_url.rstrip('/') + '/v1/audio/speech'
    
    if not api_key:
        api_key = "b77cf8cf852f4080bb56a4adcfc6a685"
    
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json",
        "Accept": "audio/mpeg"
    }

    effective_cleaning = DEFAULT_CLEANING_OPTIONS.copy()
    if cleaning_options:
        effective_cleaning.update(cleaning_options)

    data = {
        "model": "tts-1",
        "input": text,
        "voice": voice,
        "speed": speed,
        "pitch": pitch,
        "cleaning_options": effective_cleaning
    }

    if response_format:
        data["response_format"] = response_format
    
    try:
        text_length = len(text)
        print(f"â±ï¸ å›ºå®šè¶…æ—¶: {timeout_seconds}ç§’ï¼Œæ–‡æœ¬é•¿åº¦: {text_length:,} å­—ç¬¦")
        
        # å¼‚æ­¥å‘é€è¯·æ±‚å¹¶è·å–å“åº”
        timeout = aiohttp.ClientTimeout(total=timeout_seconds)
        async with session.post(api_url, headers=headers, json=data, timeout=timeout) as response:
            if response.status == 200:
                # å¼‚æ­¥è¯»å–å“åº”å†…å®¹
                content = await response.read()
                
                # ä¿å­˜éŸ³é¢‘æ–‡ä»¶
                with open(output_path, 'wb') as f:
                    f.write(content)

                # åŸºäºæ–‡æœ¬é•¿åº¦å’Œå›ºå®šé˜ˆå€¼æ£€æµ‹éŸ³é¢‘æ˜¯å¦è¿‡çŸ­/ä¸ºç©º
                expected_min_size = max(
                    MIN_AUDIO_SIZE_BYTES,
                    int(len(text) * MIN_AUDIO_BYTES_PER_CHAR)
                )
                actual_size = os.path.getsize(output_path)

                if actual_size < expected_min_size:
                    with contextlib.suppress(Exception):
                        os.remove(output_path)
                    warning_msg = (
                        f"âš ï¸ éŸ³é¢‘æ–‡ä»¶ç–‘ä¼¼å¼‚å¸¸ (å¤§å° {actual_size}B < é¢„æœŸ {expected_min_size}B, "
                        f"æ–‡æœ¬é•¿åº¦ {len(text)}). å°†è§†ä¸ºå¤±è´¥å¹¶è®¡åˆ’é‡è¯•ã€‚"
                    )
                    print(warning_msg, file=sys.stderr)
                    return False, response.status, 'audio_too_small'

                return True, response.status, None
            else:
                # å°è¯•è¯»å–é”™è¯¯å“åº”å†…å®¹
                error_detail = None
                try:
                    error_content = await response.text()
                    print(f"âŒ TTS APIè¿”å›é”™è¯¯çŠ¶æ€ç : {response.status} ({api_url})", file=sys.stderr)
                    print(f"ğŸ“„ é”™è¯¯å“åº”å†…å®¹: {error_content[:200]}...", file=sys.stderr)
                    error_detail = error_content[:200]
                except:
                    print(f"âŒ TTS APIè¿”å›é”™è¯¯çŠ¶æ€ç : {response.status} ({api_url}) - æ— æ³•è¯»å–é”™è¯¯è¯¦æƒ…", file=sys.stderr)
                    error_detail = None
                return False, response.status, error_detail
                
    except asyncio.TimeoutError:
        print(f"â° TTSè½¬æ¢è¶…æ—¶ ({api_url}) - å›ºå®šè¶…æ—¶: {timeout_seconds}s, æ–‡æœ¬é•¿åº¦: {len(text):,} å­—ç¬¦", file=sys.stderr)
        return False, None, 'timeout'
    except aiohttp.ClientConnectorError as e:
        print(f"ğŸ”Œ è¿æ¥é”™è¯¯ ({api_url}): {str(e)} - å¯èƒ½åŸå› : DNSè§£æå¤±è´¥ã€æœåŠ¡å™¨ä¸å¯è¾¾ã€ç«¯å£è¢«æ‹’ç»", file=sys.stderr)
        return False, None, str(e)
    except aiohttp.ClientError as e:
        print(f"ğŸŒ ç½‘ç»œé”™è¯¯ ({api_url}): {str(e)} - å¯èƒ½åŸå› : ç½‘ç»œä¸­æ–­ã€SSLæ¡æ‰‹å¤±è´¥", file=sys.stderr)
        return False, None, str(e)
    except Exception as e:
        print(f"ğŸ’¥ TTSè½¬æ¢å¤±è´¥ ({api_url}): {str(e)} - æœªçŸ¥é”™è¯¯", file=sys.stderr)
        return False, None, str(e)

def text_to_speech(text, output_path, voice="zh-CN-XiaoxiaoNeural", speed=1.0, api_url=None, api_key=None, pitch: float = 1.0, cleaning_options=None, response_format: str = "mp3"):
    """åŒæ­¥ç‰ˆæœ¬çš„TTSè°ƒç”¨ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰"""
    # ä½¿ç”¨ä¼ å…¥çš„APIä¿¡æ¯ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤å€¼
    if not api_url:
        api_url = "http://127.0.0.1:5050/v1/audio/speech"
    else:
        # ç¡®ä¿URLæ ¼å¼æ­£ç¡®
        if not api_url.endswith('/v1/audio/speech'):
            api_url = api_url.rstrip('/') + '/v1/audio/speech'
    
    if not api_key:
        api_key = "b77cf8cf852f4080bb56a4adcfc6a685"
    
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json",
        "Accept": "audio/mpeg"
    }

    effective_cleaning = DEFAULT_CLEANING_OPTIONS.copy()
    if cleaning_options:
        effective_cleaning.update(cleaning_options)

    data = {
        "model": "tts-1",
        "input": text,  # ç›´æ¥å‘é€åŸå§‹æ–‡æœ¬ï¼Œè®©APIç«¯å¤„ç†æ¸…ç†
        "voice": voice,
        "speed": speed,
        "pitch": pitch,
        "cleaning_options": effective_cleaning
    }

    if response_format:
        data["response_format"] = response_format
    
    try:
        # å‘é€è¯·æ±‚å¹¶è·å–å“åº”
        response = requests.post(api_url, headers=headers, json=data)
        response.raise_for_status()
        
        # ä¿å­˜éŸ³é¢‘æ–‡ä»¶
        with open(output_path, 'wb') as f:
            f.write(response.content)

        expected_min_size = max(
            MIN_AUDIO_SIZE_BYTES,
            int(len(text) * MIN_AUDIO_BYTES_PER_CHAR)
        )
        actual_size = os.path.getsize(output_path)

        if actual_size < expected_min_size:
            with contextlib.suppress(Exception):
                os.remove(output_path)
            raise ValueError(
                f"audio_too_small (size={actual_size}, expected>={expected_min_size}, text_len={len(text)})"
            )
        
        return True
    except Exception as e:
        print(f"TTSè½¬æ¢å¤±è´¥ ({api_url}): {str(e)}", file=sys.stderr)
        return False

@app.route('/')
def index():
    return render_template('index.html')

@app.route('/upload', methods=['POST'])
def upload_files():
    if 'files' not in request.files:
        return jsonify({'error': 'æ²¡æœ‰é€‰æ‹©æ–‡ä»¶'}), 400
    
    files = request.files.getlist('files')
    # è·å–å£°éŸ³å’Œè¯­é€Ÿå‚æ•°
    voice = request.form.get('voice', 'zh-CN-XiaoxiaoNeural')
    speed = float(request.form.get('speed', 1.0))
    custom_directory = request.form.get('custom_directory', '').strip()
    
    # è·å–APIæœåŠ¡å™¨ä¿¡æ¯
    api_servers_json = request.form.get('api_servers', '[]')
    concurrency = int(request.form.get('concurrency', 1))
    
    # è§£æAPIæœåŠ¡å™¨åˆ—è¡¨
    try:
        api_servers = json.loads(api_servers_json)
        # è¿‡æ»¤å‡ºå¯ç”¨çš„æœåŠ¡å™¨
        enabled_servers = [server for server in api_servers if server.get('enabled', True)]
        if not enabled_servers:
            return jsonify({'error': 'æ²¡æœ‰å¯ç”¨çš„APIæœåŠ¡å™¨'}), 400
        
        # è°ƒè¯•æ—¥å¿—ï¼šæ˜¾ç¤ºå¯ç”¨çš„æœåŠ¡å™¨
        print(f"ğŸ”§ å¯ç”¨çš„APIæœåŠ¡å™¨åˆ—è¡¨:")
        for i, server in enumerate(enabled_servers):
            print(f"  {i+1}. {server.get('name', 'Unknown')} - {server.get('url', 'No URL')}")
        print(f"ğŸ“Š æ€»å…± {len(enabled_servers)} ä¸ªå¯ç”¨çš„æœåŠ¡å™¨ï¼Œå¹¶å‘åº¦: {concurrency}")
        
    except json.JSONDecodeError:
        return jsonify({'error': 'APIæœåŠ¡å™¨é…ç½®æ ¼å¼é”™è¯¯'}), 400
    
    # ç”Ÿæˆæ‰¹é‡å¤„ç†ç›®å½•
    batch_dir = generate_batch_directory(custom_directory)
    batch_upload_dir = os.path.join(app.config['UPLOAD_FOLDER'], batch_dir)
    
    # åˆ›å»ºæ‰¹é‡å¤„ç†ç›®å½•
    os.makedirs(batch_upload_dir, exist_ok=True)
    
    # åˆå§‹åŒ–æ‰¹é‡çŠ¶æ€
    batch_id = str(uuid.uuid4())
    valid_files = [f for f in files if f and allowed_file(f.filename)]
    batch_status[batch_id] = {
        'total_files': len(valid_files),
        'completed_files': 0,
        'current_file': 0,
        'files': {},
        'server_statuses': {},  # æ·»åŠ æœåŠ¡å™¨çŠ¶æ€è·Ÿè¸ª
        'upload_dir': batch_upload_dir  # ä¿å­˜ä¸Šä¼ ç›®å½•è·¯å¾„
    }
    
    # å…ˆä¿å­˜æ‰€æœ‰æ–‡ä»¶å¹¶åˆå§‹åŒ–çŠ¶æ€
    for file in valid_files:
        filename = safe_filename(file.filename)
        file_id = f"{batch_id}_{filename}"
        
        # ä¿å­˜ä¸Šä¼ çš„Markdownæ–‡ä»¶åˆ°æ‰¹é‡ç›®å½•
        md_path = os.path.join(batch_upload_dir, filename)
        file.save(md_path)
        
        # åˆå§‹åŒ–æ–‡ä»¶çŠ¶æ€
        batch_status[batch_id]['files'][file_id] = {
            'filename': filename,
            'status': 'waiting',
            'progress': 0,
            'stage': 'ç­‰å¾…å¤„ç†'
        }
    
    # å¯åŠ¨åå°å¤„ç†ä»»åŠ¡
    import threading
    thread = threading.Thread(target=run_async_processing, args=(batch_id, batch_upload_dir, voice, speed, enabled_servers, concurrency))
    thread.daemon = True
    thread.start()
    
    return jsonify({
        'batch_id': batch_id,
        'batch_directory': batch_dir,
        'total_files': len(valid_files)
    })

def run_async_processing(batch_id, batch_upload_dir, voice, speed, api_servers, concurrency, specific_files=None):
    """è¿è¡Œå¼‚æ­¥å¤„ç†çš„ä¸»å‡½æ•°"""
    try:
        # åˆ›å»ºæ–°çš„äº‹ä»¶å¾ªç¯
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        
        # è¿è¡Œå¼‚æ­¥å¤„ç†
        loop.run_until_complete(process_files_async(batch_id, batch_upload_dir, voice, speed, api_servers, concurrency, specific_files))
        
    except Exception as e:
        print(f"å¼‚æ­¥å¤„ç†å¼‚å¸¸: {str(e)}", file=sys.stderr)
    finally:
        loop.close()

async def process_files_async(batch_id, batch_upload_dir, voice, speed, api_servers, concurrency, specific_files=None):
    """å¼‚æ­¥å¤„ç†æ–‡ä»¶ï¼Œæ”¯æŒé€‰æ‹©è´Ÿè½½å‡è¡¡å™¨"""
    if batch_id not in batch_status:
        return
    
    batch_info = batch_status[batch_id]
    
    # æ ¹æ®é…ç½®é€‰æ‹©ä½¿ç”¨å“ªä¸ªè´Ÿè½½å‡è¡¡å™¨
    if USE_SIMPLE_BALANCER:
        print("âš¡ ä½¿ç”¨è°ƒåº¦å®˜è´Ÿè½½å‡è¡¡å™¨ (V5)")
        await dispatcher_balancer_v5(batch_id, batch_upload_dir, voice, speed, api_servers, concurrency, specific_files)
        return
    
    # å¦‚æœæŒ‡å®šäº†ç‰¹å®šæ–‡ä»¶ï¼Œåªå¤„ç†è¿™äº›æ–‡ä»¶ï¼›å¦åˆ™å¤„ç†æ‰€æœ‰æ–‡ä»¶
    if specific_files:
        files_to_process = specific_files
        print(f"ğŸ”„ é‡è¯•æ¨¡å¼: åªå¤„ç†æŒ‡å®šçš„ {len(files_to_process)} ä¸ªæ–‡ä»¶")
    else:
        files_to_process = list(batch_info['files'].keys())
        print(f"ğŸ†• å…¨æ–°å¤„ç†: å¤„ç†æ‰€æœ‰ {len(files_to_process)} ä¸ªæ–‡ä»¶")
    
    # è°ƒè¯•æ—¥å¿—ï¼šæ˜¾ç¤ºå¼‚æ­¥å¤„ç†é…ç½®
    print(f"ğŸš€ å¼€å§‹åŠ¨æ€è´Ÿè½½å‡è¡¡å¤„ç†:")
    print(f"  ğŸ“ æ‰¹æ¬¡ID: {batch_id}")
    print(f"  ğŸ“„ æ–‡ä»¶æ•°é‡: {len(files_to_process)}")
    print(f"  ğŸ–¥ï¸ å¯ç”¨æœåŠ¡å™¨: {len(api_servers)}")
    print(f"  âš¡ å¹¶å‘åº¦: {concurrency}")
    
    # åˆ›å»ºaiohttpä¼šè¯ï¼Œä¼˜åŒ–è¿æ¥æ± è®¾ç½®
    connector = aiohttp.TCPConnector(
        limit=concurrency * 3,  # æ€»è¿æ¥æ± é™åˆ¶
        limit_per_host=concurrency * 2,  # æ¯ä¸ªä¸»æœºçš„è¿æ¥é™åˆ¶
        keepalive_timeout=60,  # ä¿æŒè¿æ¥60ç§’
        enable_cleanup_closed=True  # è‡ªåŠ¨æ¸…ç†å…³é—­çš„è¿æ¥
    )
    # ä¼šè¯çº§åˆ«çš„è¶…æ—¶è®¾ç½®æ›´å®½æ¾ï¼Œå› ä¸ºå•ä¸ªè¯·æ±‚çš„è¶…æ—¶ç”±è¯·æ±‚çº§åˆ«æ§åˆ¶
    timeout = aiohttp.ClientTimeout(total=1800)  # 30åˆ†é’Ÿæ€»è¶…æ—¶ï¼ˆä¼šè¯çº§åˆ«ï¼‰
    
    async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:
        # åˆ›å»ºä»»åŠ¡é˜Ÿåˆ—å’ŒæœåŠ¡å™¨çŠ¶æ€è·Ÿè¸ª
        task_queue = asyncio.Queue()
        server_stats = {i: {'active_tasks': 0, 'completed_tasks': 0, 'total_time': 0} for i in range(len(api_servers))}
        
        # åˆå§‹åŒ–æœåŠ¡å™¨çŠ¶æ€
        for i in range(len(api_servers)):
            batch_info['server_statuses'][i] = {
                'name': api_servers[i]['name'],
                'status': 'idle',
                'load': 0,
                'max_load': concurrency,
                'completed_tasks': 0,
                'total_time': 0
            }
        
        # å°†æ‰€æœ‰æ–‡ä»¶æ·»åŠ åˆ°é˜Ÿåˆ—
        for file_id in files_to_process:
            await task_queue.put(file_id)
        
        print(f"ğŸ“¤ åˆ›å»ºä»»åŠ¡é˜Ÿåˆ—: {len(files_to_process)} ä¸ªæ–‡ä»¶")
        
        # åˆ›å»ºçœŸæ­£çš„åŠ¨æ€è´Ÿè½½å‡è¡¡å¤„ç†å™¨
        async def dynamic_load_balancer():
            """çœŸæ­£çš„åŠ¨æ€è´Ÿè½½å‡è¡¡å¤„ç†å™¨ - ä»»åŠ¡å®Œæˆå³åˆ†é…æ–°ä»»åŠ¡ï¼Œå¤±è´¥ä»»åŠ¡è‡ªåŠ¨é‡è¯•"""
            start_time = time.time()
            total_tasks = len(files_to_process)
            completed_tasks = 0
            failed_tasks = []
            retry_queue = asyncio.Queue()  # å¤±è´¥ä»»åŠ¡é‡è¯•é˜Ÿåˆ—
            
            print(f"ğŸš€ å¯åŠ¨åŠ¨æ€è´Ÿè½½å‡è¡¡å™¨:")
            print(f"  ğŸ“Š æ€»ä»»åŠ¡æ•°: {total_tasks}")
            print(f"  ğŸ–¥ï¸ å¯ç”¨æœåŠ¡å™¨: {len(api_servers)}")
            print(f"  âš¡ æ¯æœåŠ¡å™¨å¹¶å‘åº¦: {concurrency}")
            print(f"  ğŸ¯ ç†è®ºæœ€å¤§å¹¶å‘: {len(api_servers) * concurrency}")
            
            # åˆ›å»ºä»»åŠ¡å®Œæˆå›è°ƒå‡½æ•°
            async def on_task_completed(file_id, server_id, success, processing_time):
                nonlocal completed_tasks
                
                # æ›´æ–°æœåŠ¡å™¨ç»Ÿè®¡
                server_stats[server_id]['active_tasks'] -= 1
                server_stats[server_id]['total_time'] += processing_time
                
                # æ›´æ–°æœåŠ¡å™¨çŠ¶æ€
                current_load = server_stats[server_id]['active_tasks']
                batch_info['server_statuses'][server_id]['load'] = current_load
                batch_info['server_statuses'][server_id]['total_time'] = server_stats[server_id]['total_time']
                
                if current_load == 0:
                    batch_info['server_statuses'][server_id]['status'] = 'idle'
                elif current_load >= concurrency:
                    batch_info['server_statuses'][server_id]['status'] = 'full'
                else:
                    batch_info['server_statuses'][server_id]['status'] = 'busy'
                
                # æ£€æŸ¥æ˜¯å¦è¶…æ—¶ï¼ˆè¶…è¿‡5åˆ†é’Ÿè®¤ä¸ºè¶…æ—¶ï¼‰
                if processing_time > 300:
                    print(f"â° ä»»åŠ¡è¶…æ—¶æ£€æµ‹: {file_id} è€—æ—¶ {processing_time:.2f}ç§’ï¼Œå¯èƒ½å­˜åœ¨é—®é¢˜")
                
                if success:
                    server_stats[server_id]['completed_tasks'] += 1
                    batch_info['server_statuses'][server_id]['completed_tasks'] = server_stats[server_id]['completed_tasks']
                    completed_tasks += 1
                    # æ›´æ–°æ‰¹æ¬¡å®Œæˆè®¡æ•°
                    batch_info['completed_files'] = completed_tasks
                    batch_info['current_file'] = completed_tasks
                    print(f"âœ… ä»»åŠ¡å®Œæˆ: {file_id} (æœåŠ¡å™¨: {api_servers[server_id]['name']}, è€—æ—¶: {processing_time:.2f}ç§’)")
                else:
                    # ä»»åŠ¡å¤±è´¥ï¼ŒåŠ å…¥é‡è¯•é˜Ÿåˆ—ï¼ˆåŒ…å«å¤±è´¥æœåŠ¡å™¨ä¿¡æ¯ï¼‰
                    failed_tasks.append(file_id)
                    retry_info = {
                        'file_id': file_id,
                        'failed_server_id': server_id,
                        'failed_server_name': api_servers[server_id]['name']
                    }
                    await retry_queue.put(retry_info)
                    print(f"âŒ ä»»åŠ¡å¤±è´¥: {file_id} (æœåŠ¡å™¨: {api_servers[server_id]['name']}, è€—æ—¶: {processing_time:.2f}ç§’) - åŠ å…¥é‡è¯•é˜Ÿåˆ—")
                
                # ç«‹å³å°è¯•åˆ†é…æ–°ä»»åŠ¡ç»™è¿™ä¸ªæœåŠ¡å™¨
                print(f"ğŸ¯ ä»»åŠ¡å®Œæˆï¼Œæ£€æŸ¥æœåŠ¡å™¨ {api_servers[server_id]['name']} æ˜¯å¦å¯ä»¥æ¥æ”¶æ–°ä»»åŠ¡ (å½“å‰è´Ÿè½½: {current_load}/{concurrency})")
                print(f"ğŸ“Š é˜Ÿåˆ—çŠ¶æ€: ä¸»é˜Ÿåˆ—={task_queue.qsize()}, é‡è¯•é˜Ÿåˆ—={retry_queue.qsize()}, å·²å®Œæˆ={completed_tasks}/{total_tasks}")
                await assign_next_task(server_id)
                
                # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰ä»»åŠ¡éƒ½å·²å®Œæˆ
                if completed_tasks >= total_tasks:
                    print(f"ğŸ‰ æ‰€æœ‰ä»»åŠ¡å·²å®Œæˆï¼")
                    return
            
            # ä»»åŠ¡åˆ†é…å‡½æ•°
            async def assign_next_task(server_id):
                """ä¸ºæŒ‡å®šæœåŠ¡å™¨åˆ†é…ä¸‹ä¸€ä¸ªä»»åŠ¡"""
                current_load = server_stats[server_id]['active_tasks']
                server_name = api_servers[server_id]['name']
                
                print(f"ğŸ” æ£€æŸ¥æœåŠ¡å™¨ {server_name} ä»»åŠ¡åˆ†é… (å½“å‰è´Ÿè½½: {current_load}/{concurrency})")
                
                if current_load >= concurrency:
                    print(f"âš ï¸ æœåŠ¡å™¨ {server_name} å·²æ»¡ï¼Œè·³è¿‡ä»»åŠ¡åˆ†é…")
                    return  # æœåŠ¡å™¨å·²æ»¡
                
                # ä¼˜å…ˆä»é‡è¯•é˜Ÿåˆ—è·å–å¤±è´¥çš„ä»»åŠ¡
                file_id = None
                if not retry_queue.empty():
                    try:
                        retry_info = retry_queue.get_nowait()
                        file_id = retry_info['file_id']
                        failed_server_id = retry_info['failed_server_id']
                        failed_server_name = retry_info['failed_server_name']
                        
                        # æ£€æŸ¥æ˜¯å¦åˆ†é…ç»™ä¸åŒçš„æœåŠ¡å™¨
                        if server_id != failed_server_id:
                            print(f"ğŸ”„ é‡è¯•ä»»åŠ¡: {file_id} â†’ {server_name} (åŸå¤±è´¥æœåŠ¡å™¨: {failed_server_name})")
                        else:
                            # å¦‚æœè¿˜æ˜¯åŒä¸€ä¸ªæœåŠ¡å™¨ï¼Œæ”¾å›é˜Ÿåˆ—ç­‰å¾…å…¶ä»–æœåŠ¡å™¨
                            await retry_queue.put(retry_info)
                            file_id = None
                            print(f"âš ï¸ è·³è¿‡é‡è¯•: {file_id} é¿å…åˆ†é…ç»™åŒä¸€å¤±è´¥æœåŠ¡å™¨ {failed_server_name}")
                    except asyncio.QueueEmpty:
                        pass
                
                # å¦‚æœé‡è¯•é˜Ÿåˆ—ä¸ºç©ºï¼Œä»ä¸»é˜Ÿåˆ—è·å–
                if file_id is None and not task_queue.empty():
                    try:
                        file_id = task_queue.get_nowait()
                        print(f"ğŸ“¤ æ–°ä»»åŠ¡: {file_id} â†’ {server_name}")
                    except asyncio.QueueEmpty:
                        print(f"ğŸ“­ æœåŠ¡å™¨ {server_name} æ— ä»»åŠ¡å¯åˆ†é…")
                        return
                
                if file_id is not None:
                    # æ›´æ–°æœåŠ¡å™¨çŠ¶æ€
                    server_stats[server_id]['active_tasks'] += 1
                    current_load = server_stats[server_id]['active_tasks']
                    batch_info['server_statuses'][server_id]['load'] = current_load
                    
                    if current_load >= concurrency:
                        batch_info['server_statuses'][server_id]['status'] = 'full'
                    else:
                        batch_info['server_statuses'][server_id]['status'] = 'busy'
                    
                    server_name = api_servers[server_id]['name']
                    print(f"ğŸ¯ åŠ¨æ€åˆ†é…: {file_id} â†’ {server_name} (è´Ÿè½½:{current_load}/{concurrency})")
                    print(f"âœ… ä»»åŠ¡åˆ†é…ç¡®è®¤: {file_id} å·²æˆåŠŸåˆ†é…ç»™ {server_name}")
                    
                    # å¯åŠ¨ä»»åŠ¡
                    task = asyncio.create_task(
                        process_single_file_with_callback(
                            session, batch_id, batch_upload_dir, voice, speed, 
                            api_servers, file_id, server_id, server_stats, concurrency,
                            on_task_completed
                        )
                    )
                else:
                    # æ²¡æœ‰ä»»åŠ¡å¯åˆ†é…
                    print(f"ğŸ“­ æœåŠ¡å™¨ {server_name} æ— ä»»åŠ¡å¯åˆ†é… (é‡è¯•é˜Ÿåˆ—: {retry_queue.qsize()}, ä¸»é˜Ÿåˆ—: {task_queue.qsize()})")
                    
                    # å¦‚æœæ‰€æœ‰é˜Ÿåˆ—éƒ½ä¸ºç©ºï¼Œæ£€æŸ¥æ˜¯å¦å¯ä»¥é€€å‡º
                    if task_queue.empty() and retry_queue.empty():
                        active_tasks = sum(server_stats[i]['active_tasks'] for i in range(len(api_servers)))
                        if active_tasks == 0:
                            print(f"ğŸ¯ æ‰€æœ‰ä»»åŠ¡å·²å®Œæˆï¼Œæ‰€æœ‰æœåŠ¡å™¨ç©ºé—²")
                            return
            
            # åˆå§‹åˆ†é…ï¼šä¸ºæ‰€æœ‰æœåŠ¡å™¨åˆ†é…åˆå§‹ä»»åŠ¡
            print(f"ğŸš€ å¼€å§‹åˆå§‹ä»»åŠ¡åˆ†é…...")
            for server_id in range(len(api_servers)):
                for _ in range(min(concurrency, total_tasks)):
                    await assign_next_task(server_id)
                    if task_queue.empty():
                        break
                print(f"  ğŸ“¤ æœåŠ¡å™¨ {api_servers[server_id]['name']} åˆå§‹åˆ†é…å®Œæˆï¼Œå½“å‰è´Ÿè½½: {server_stats[server_id]['active_tasks']}")
            
            print(f"ğŸ“Š åˆå§‹åˆ†é…å®Œæˆï¼Œå‰©ä½™é˜Ÿåˆ—ä»»åŠ¡: {task_queue.qsize()}")
            
            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼ˆäº‹ä»¶é©±åŠ¨ï¼Œæ— éœ€è½®è¯¢ï¼‰
            print(f"ğŸ¯ å¯åŠ¨äº‹ä»¶é©±åŠ¨è´Ÿè½½å‡è¡¡ï¼Œç­‰å¾…ä»»åŠ¡å®Œæˆ...")
            
            # åˆ›å»ºä»»åŠ¡å®Œæˆç­‰å¾…å™¨
            async def wait_for_completion():
                while completed_tasks < total_tasks:
                    # æ£€æŸ¥æ˜¯å¦æœ‰æ´»è·ƒä»»åŠ¡
                    active_tasks = sum(server_stats[i]['active_tasks'] for i in range(len(api_servers)))
                    if active_tasks == 0 and task_queue.empty() and retry_queue.empty():
                        print(f"âš ï¸ æ‰€æœ‰ä»»åŠ¡å·²å®Œæˆä½†è®¡æ•°ä¸åŒ¹é…ï¼Œå¼ºåˆ¶é€€å‡º")
                        break
                    await asyncio.sleep(0.5)  # å‡å°‘æ£€æŸ¥é¢‘ç‡
            
            await wait_for_completion()
            
            total_time = time.time() - start_time
            print(f"ğŸ‰ åŠ¨æ€è´Ÿè½½å‡è¡¡å¤„ç†å®Œæˆ (æ€»è€—æ—¶: {total_time:.2f}ç§’)")
            print(f"ğŸ“Š æœ€ç»ˆç»Ÿè®¡: å®Œæˆ {completed_tasks}/{total_tasks} ä¸ªä»»åŠ¡")
            
            # è¾“å‡ºè¯¦ç»†çš„æœåŠ¡å™¨ç»Ÿè®¡ä¿¡æ¯
            print(f"ğŸ“Š æœåŠ¡å™¨æ€§èƒ½ç»Ÿè®¡:")
            for i, stats in server_stats.items():
                server_name = api_servers[i]['name']
                if stats['completed_tasks'] > 0:
                    avg_time = stats['total_time'] / stats['completed_tasks']
                    throughput = stats['completed_tasks'] / total_time if total_time > 0 else 0
                    print(f"  ğŸ–¥ï¸ {server_name}:")
                    print(f"    âœ… å®Œæˆä»»åŠ¡: {stats['completed_tasks']} ä¸ª")
                    print(f"    â±ï¸ å¹³å‡è€—æ—¶: {avg_time:.2f}ç§’/ä»»åŠ¡")
                    print(f"    ğŸš€ ååé‡: {throughput:.2f}ä»»åŠ¡/ç§’")
                    print(f"    ğŸ“ˆ æ•ˆç‡è¯„åˆ†: {1.0/max(avg_time, 0.1):.2f}")
                else:
                    print(f"  ğŸ–¥ï¸ {server_name}: æœªå¤„ç†ä»»åŠ¡")
        
        # è¿è¡ŒåŠ¨æ€è´Ÿè½½å‡è¡¡å¤„ç†å™¨
        await dynamic_load_balancer()
        
        # ç»Ÿè®¡æœ€ç»ˆç»“æœ
        success_count = sum(1 for file_id in files_to_process 
                           if batch_info['files'][file_id]['status'] == 'completed')
        
        print(f"ğŸ‰ å¼‚æ­¥å¤„ç†å®Œæˆ: {success_count}/{len(files_to_process)} ä¸ªæ–‡ä»¶æˆåŠŸ")
        print(f"ğŸ“Š ä½¿ç”¨äº† {len(api_servers)} ä¸ªæœåŠ¡å™¨ï¼Œå¹¶å‘åº¦: {concurrency}")

async def dynamic_worker_balancer_v4(batch_id, batch_upload_dir, voice, speed, api_servers, specific_files=None):
    """V4ï¼šåŸºäºæŒä¹…åŒ–å·¥ä½œèŠ‚ç‚¹ä¸é˜Ÿåˆ—çš„äº‹ä»¶é©±åŠ¨åŠ¨æ€è´Ÿè½½å‡è¡¡ã€‚"""
    if batch_id not in batch_status:
        return

    batch_info = batch_status[batch_id]

    # 1) ä»»åŠ¡åˆ—è¡¨
    files_to_process = specific_files or list(batch_info['files'].keys())
    total_tasks_count = len(files_to_process)

    warmup_primary = max(10, MAX_CONCURRENCY * 2)
    warmup_secondary = max(10, MAX_CONCURRENCY)
    WARMUP_COUNT = min(total_tasks_count, warmup_primary)
    SECOND_STAGE_COUNT = max(0, min(total_tasks_count - WARMUP_COUNT, warmup_secondary))

    print("ğŸš€ å¯åŠ¨åŠ¨æ€å·¥ä½œèŠ‚ç‚¹è´Ÿè½½å‡è¡¡å™¨ (V4):")
    print(f"  ğŸ“Š æ€»ä»»åŠ¡æ•°: {total_tasks_count}")
    print(f"  ğŸ–¥ï¸ æœåŠ¡å™¨èŠ‚ç‚¹æ•°: {len(api_servers)}")

    # åˆå§‹åŒ– WebUI æœåŠ¡å™¨çŠ¶æ€
    batch_info['server_statuses'] = {}
    for i, server in enumerate(api_servers):
        batch_info['server_statuses'][i] = {
            'name': server.get('name', f'Server-{i}'),
            'status': 'idle',
            'load': 0,
            'max_load': 1,
            'completed_tasks': 0,
            'timeout_tasks': 0,
            'failed_tasks': 0,
            'total_time': 0.0,
        }

    # 2) é˜Ÿåˆ—ï¼šæ”¾ (file_id, retry_count)
    MAX_RETRIES = 3
    task_queue = asyncio.Queue()
    for file_id in files_to_process:
        task_queue.put_nowait((file_id, 0))

    # åœæ­¢ä¿¡å·ï¼ˆæ¯ä¸ªå·¥ä½œèŠ‚ç‚¹ä¸€ä¸ªï¼‰
    STOP_SIGNAL = object()
    for _ in range(len(api_servers)):
        task_queue.put_nowait(STOP_SIGNAL)

    batch_info['completed_files'] = 0

    # 3) å·¥ä½œèŠ‚ç‚¹å®šä¹‰
    async def worker_node(server_id, server_info):
        server_name = server_info.get('name', f"Server-{server_id}")
        server_url = server_info.get('url')
        api_key = server_info.get('apiKey', server_info.get('api_key', ''))
        stats = {'success': 0, 'fail': 0, 'total_time': 0.0}

        print(f"ğŸ‘· å·¥ä½œèŠ‚ç‚¹ {server_name} å·²å¯åŠ¨å¹¶å¾…å‘½ã€‚")

        async with aiohttp.ClientSession() as session:
            while True:
                task = await task_queue.get()

                if task is STOP_SIGNAL:
                    task_queue.task_done()
                    break

                file_id, retry_count = task
                if batch_id not in batch_status or file_id not in batch_status[batch_id]['files']:
                    task_queue.task_done()
                    continue

                filename = batch_info['files'][file_id]['filename']
                batch_info['files'][file_id]['stage'] = f'å¤„ç†ä¸­ @{server_name}'
                batch_info['files'][file_id]['status'] = 'processing'

                # WebUIï¼šèŠ‚ç‚¹è´Ÿè½½æ›´æ–°
                batch_info['server_statuses'][server_id]['status'] = 'busy'
                batch_info['server_statuses'][server_id]['load'] = 1

                print(f"  -> èŠ‚ç‚¹ {server_name} æ¥æ”¶ä»»åŠ¡: {filename} (ç¬¬ {retry_count + 1} æ¬¡å°è¯•)")

                start_time = time.time()
                success = False
                status_code = None
                try:
                    input_path = os.path.join(batch_upload_dir, filename)
                    output_path = os.path.join(batch_upload_dir, filename.replace('.md', '.mp3'))
                    with open(input_path, 'r', encoding='utf-8') as f:
                        text = f.read()

                    # è‹¥è®¾ç½®GLOBAL_CONCURRENCY_LIMIT>0ï¼Œåˆ™å¯ç”¨æ€»é—¸é—¨ï¼›å¦åˆ™ç›´æ¥è°ƒç”¨
                    if global_api_semaphore is not None:
                        async with global_api_semaphore:
                            success, status_code, error_detail = await async_text_to_speech(
                                session, text, output_path, voice, speed, server_url, api_key, timeout_seconds=300
                            )
                    else:
                        success, status_code, error_detail = await async_text_to_speech(
                            session, text, output_path, voice, speed, server_url, api_key, timeout_seconds=300
                        )
                    if not success:
                        raise Exception("APIè¿”å›é200çŠ¶æ€ç ")

                except Exception as e:
                    if status_code is not None:
                        print(f"âŒ èŠ‚ç‚¹ {server_name} ä»»åŠ¡å¤±è´¥: {filename}, çŠ¶æ€ç : {status_code}, åŸå› : {type(e).__name__}")
                    else:
                        print(f"âŒ èŠ‚ç‚¹ {server_name} ä»»åŠ¡å¤±è´¥: {filename}, åŸå› : {type(e).__name__}")

                processing_time = time.time() - start_time

                # ç»Ÿè®¡ä¸UIæ›´æ–°
                batch_info['server_statuses'][server_id]['total_time'] += processing_time

                if success:
                    stats['success'] += 1
                    stats['total_time'] += processing_time
                    batch_info['files'][file_id]['status'] = 'completed'
                    batch_info['files'][file_id]['stage'] = 'âœ… å®Œæˆ'
                    batch_info['completed_files'] += 1
                    batch_info['current_file'] = batch_info['completed_files']
                    batch_info['server_statuses'][server_id]['completed_tasks'] += 1
                    print(f"âœ… ä»»åŠ¡å®Œæˆ: {filename} (æœåŠ¡å™¨: {server_name}, è€—æ—¶: {processing_time:.2f}ç§’)")
                else:
                    stats['fail'] += 1
                    if retry_count < MAX_RETRIES:
                        # æŒ‡æ•°é€€é¿ + æŠ–åŠ¨
                        delay = (2 ** retry_count) + random.uniform(0, 1)
                        print(f"ğŸ”„ ä»»åŠ¡å°†é‡è¯• (ç¬¬ {retry_count+1} æ¬¡)ï¼Œå°†åœ¨ {delay:.2f} ç§’åæ‰§è¡Œ... [{filename}]")
                        # å»¶è¿Ÿåæ”¾å›é˜Ÿåˆ—ï¼Œä¸ç»‘å®šåŒä¸€èŠ‚ç‚¹
                        async def requeue_after_delay(delay_s: float, item):
                            await asyncio.sleep(delay_s)
                            await task_queue.put(item)
                        asyncio.create_task(requeue_after_delay(delay, (file_id, retry_count + 1)))
                        batch_info['files'][file_id]['stage'] = f'ç­‰å¾…é‡è¯• ({retry_count+1}/{MAX_RETRIES})'
                    else:
                        batch_info['files'][file_id]['status'] = 'failed'
                        batch_info['files'][file_id]['stage'] = 'âŒ å¤±è´¥ (å·²è¾¾ä¸Šé™)'
                        batch_info['completed_files'] += 1
                        batch_info['current_file'] = batch_info['completed_files']

                task_queue.task_done()
                # WebUIï¼šèŠ‚ç‚¹æ¢å¤ç©ºé—²
                batch_info['server_statuses'][server_id]['load'] = 0
                batch_info['server_statuses'][server_id]['status'] = 'idle'

        print(f"ğŸ èŠ‚ç‚¹ {server_name} åœæ­¢ã€‚ç»Ÿè®¡: æˆåŠŸ {stats['success']}, å¤±è´¥ {stats['fail']}.")

    # 4) å¯åŠ¨æ‰€æœ‰å·¥ä½œèŠ‚ç‚¹ï¼ˆæ¯å°æœåŠ¡å™¨ä¸€ä¸ªå¹¶å‘=1çš„èŠ‚ç‚¹ï¼‰
    worker_tasks = [asyncio.create_task(worker_node(i, s)) for i, s in enumerate(api_servers)]

    # 5) ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
    await task_queue.join()
    await asyncio.gather(*worker_tasks)

    print("ğŸ‰ V4 åŠ¨æ€è´Ÿè½½å‡è¡¡å™¨å¤„ç†å®Œæˆï¼")

async def dynamic_worker_balancer_v4_1(batch_id, batch_upload_dir, voice, speed, api_servers, specific_files=None):
    """V4.1ï¼šä¿®å¤å·¥ä½œèŠ‚ç‚¹ç”Ÿå‘½å‘¨æœŸï¼Œä½¿ç”¨stop_eventå®ç°åŠ¨æ€ç«äº‰ä¸ä¼˜é›…é€€å‡ºã€‚"""
    if batch_id not in batch_status:
        return

    batch_info = batch_status[batch_id]

    MAX_RETRIES = 3
    files_to_process = specific_files or list(batch_info['files'].keys())
    total_tasks_count = len(files_to_process)

    print("ğŸš€ å¯åŠ¨åŠ¨æ€å·¥ä½œèŠ‚ç‚¹è´Ÿè½½å‡è¡¡å™¨ (V4.1 - è¡¥ä¸ç‰ˆ):")
    print(f"  ğŸ“Š æ€»ä»»åŠ¡æ•°: {total_tasks_count}")
    print(f"  ğŸ–¥ï¸ æœåŠ¡å™¨èŠ‚ç‚¹æ•°: {len(api_servers)}")

    # åˆå§‹åŒ– WebUI æœåŠ¡å™¨çŠ¶æ€
    batch_info['server_statuses'] = {}
    for i, server in enumerate(api_servers):
        batch_info['server_statuses'][i] = {
            'name': server.get('name', f'Server-{i}'),
            'status': 'idle',
            'load': 0,
            'max_load': 1,
            'completed_tasks': 0,
            'timeout_tasks': 0,
            'failed_tasks': 0,
            'total_time': 0.0,
        }

    # ä»»åŠ¡é˜Ÿåˆ—ï¼šä»…æ”¾æ™®é€šä»»åŠ¡
    task_queue = asyncio.Queue()
    for file_id in files_to_process:
        task_queue.put_nowait((file_id, 0))

    # åœæ­¢äº‹ä»¶ï¼šç”±ç›‘æ§åç¨‹åœ¨é˜Ÿåˆ—å®Œæˆåç»Ÿä¸€å‘å‡º
    stop_event = asyncio.Event()

    batch_info['completed_files'] = 0

    async def worker_node(server_id, server_info):
        server_name = server_info.get('name', f"Server-{server_id}")
        server_url = server_info.get('url')
        api_key = server_info.get('apiKey', server_info.get('api_key', ''))
        stats = {'success': 0, 'fail': 0, 'total_time': 0.0}

        print(f"ğŸ‘· å·¥ä½œèŠ‚ç‚¹ {server_name} å·²å¯åŠ¨å¹¶è¿›å…¥ç›‘å¬å¾ªç¯ã€‚")

        async with aiohttp.ClientSession() as session:
            while not stop_event.is_set():
                try:
                    task = await asyncio.wait_for(task_queue.get(), timeout=1.0)
                except asyncio.TimeoutError:
                    continue
                except asyncio.CancelledError:
                    break

                # è§£æä»»åŠ¡
                try:
                    file_id, retry_count = task
                except Exception:
                    task_queue.task_done()
                    continue

                if batch_id not in batch_status or file_id not in batch_status[batch_id]['files']:
                    task_queue.task_done()
                    continue

                filename = batch_info['files'][file_id]['filename']
                batch_info['files'][file_id]['stage'] = f'å¤„ç†ä¸­ @{server_name}'
                batch_info['files'][file_id]['status'] = 'processing'

                # WebUIï¼šèŠ‚ç‚¹ç½®ä¸ºå¿™ç¢Œ
                batch_info['server_statuses'][server_id]['status'] = 'busy'
                batch_info['server_statuses'][server_id]['load'] = 1

                print(f"  -> èŠ‚ç‚¹ {server_name} æ¥æ”¶ä»»åŠ¡: {filename} (ç¬¬ {retry_count + 1} æ¬¡å°è¯•)")

                start_time = time.time()
                success = False
                status_code = None
                try:
                    input_path = os.path.join(batch_upload_dir, filename)
                    output_path = os.path.join(batch_upload_dir, filename.replace('.md', '.mp3'))
                    with open(input_path, 'r', encoding='utf-8') as f:
                        text = f.read()

                    success, status_code, error_detail = await async_text_to_speech(
                        session, text, output_path, voice, speed, server_url, api_key, timeout_seconds=300
                    )
                    if not success:
                        raise Exception("APIè¿”å›é200çŠ¶æ€ç ")

                except Exception as e:
                    if status_code is not None:
                        print(f"âŒ èŠ‚ç‚¹ {server_name} ä»»åŠ¡å¤±è´¥: {filename}, çŠ¶æ€ç : {status_code}, åŸå› : {type(e).__name__}")
                    else:
                        print(f"âŒ èŠ‚ç‚¹ {server_name} ä»»åŠ¡å¤±è´¥: {filename}, åŸå› : {type(e).__name__}")

                processing_time = time.time() - start_time

                # ç»Ÿè®¡æ—¶é—´
                batch_info['server_statuses'][server_id]['total_time'] += processing_time

                if success:
                    stats['success'] += 1
                    stats['total_time'] += processing_time
                    batch_info['files'][file_id]['status'] = 'completed'
                    batch_info['files'][file_id]['stage'] = 'âœ… å®Œæˆ'
                    batch_info['completed_files'] += 1
                    batch_info['current_file'] = batch_info['completed_files']
                    batch_info['server_statuses'][server_id]['completed_tasks'] += 1
                    print(f"âœ… ä»»åŠ¡å®Œæˆ: {filename} (æœåŠ¡å™¨: {server_name}, è€—æ—¶: {processing_time:.2f}ç§’)")
                else:
                    stats['fail'] += 1
                    if retry_count < MAX_RETRIES:
                        delay = (2 ** retry_count) + random.uniform(0, 1)
                        print(f"ğŸ”„ ä»»åŠ¡å°†é‡è¯• (ç¬¬ {retry_count+1} æ¬¡)ï¼Œå°†åœ¨ {delay:.2f} ç§’åæ‰§è¡Œ... [{filename}]")
                        async def requeue_after_delay(delay_s: float, item):
                            await asyncio.sleep(delay_s)
                            await task_queue.put(item)
                        asyncio.create_task(requeue_after_delay(delay, (file_id, retry_count + 1)))
                        batch_info['files'][file_id]['stage'] = f'ç­‰å¾…é‡è¯• ({retry_count+1}/{MAX_RETRIES})'
                    else:
                        batch_info['files'][file_id]['status'] = 'failed'
                        batch_info['files'][file_id]['stage'] = 'âŒ å¤±è´¥ (å·²è¾¾ä¸Šé™)'
                        batch_info['completed_files'] += 1
                        batch_info['current_file'] = batch_info['completed_files']

                # æ ‡è®°ä»»åŠ¡å¤„ç†å®Œæˆï¼ˆæ— è®ºæˆåŠŸå¤±è´¥ï¼‰
                task_queue.task_done()
                # èŠ‚ç‚¹æ¢å¤ç©ºé—²
                batch_info['server_statuses'][server_id]['load'] = 0
                batch_info['server_statuses'][server_id]['status'] = 'idle'

        print(f"ğŸ èŠ‚ç‚¹ {server_name} æ”¶åˆ°åœæ­¢ä¿¡å·å¹¶é€€å‡ºã€‚ç»Ÿè®¡: æˆåŠŸ {stats['success']}, å¤±è´¥ {stats['fail']}")

    # å¯åŠ¨æ‰€æœ‰å·¥ä½œèŠ‚ç‚¹
    worker_tasks = [asyncio.create_task(worker_node(i, s)) for i, s in enumerate(api_servers)]

    # ç›‘æ§å®Œæˆï¼šé˜Ÿåˆ—æ¸…ç©ºåå‘å‡ºåœæ­¢äº‹ä»¶
    async def monitor_completion():
        await task_queue.join()
        print("âœ… æ‰€æœ‰ä»»åŠ¡å·²å¤„ç†å®Œæˆï¼Œå‘æ‰€æœ‰å·¥ä½œèŠ‚ç‚¹å‘é€åœæ­¢ä¿¡å·...")
        stop_event.set()

    await monitor_completion()

    # ç­‰å¾…æ‰€æœ‰å·¥ä½œè€…ä¼˜é›…é€€å‡º
    await asyncio.gather(*worker_tasks, return_exceptions=True)

    print("ğŸ‰ V4.1 è´Ÿè½½å‡è¡¡å™¨å¤„ç†å®Œæˆï¼")

async def dispatcher_balancer_v5(batch_id, batch_upload_dir, voice, speed, api_servers, concurrency, specific_files=None):
    return await dispatcher_balancer_v5_1(batch_id, batch_upload_dir, voice, speed, api_servers, concurrency, specific_files)


async def dispatcher_balancer_v5_1(batch_id, batch_upload_dir, voice, speed, api_servers, concurrency, specific_files=None):
    """V5.1ï¼šè°ƒåº¦å®˜æ¨¡å‹å‡çº§ç‰ˆï¼ŒåŒ…å«é¢„çƒ­ä¸è‡ªé€‚åº”é€Ÿç‡æ§åˆ¶ã€‚"""
    if batch_id not in batch_status:
        return

    batch_info = batch_status[batch_id]

    # --- 1. å…³é”®å‚æ•° ---
    total_workers = max(1, len(api_servers))

    env_limit_raw = os.environ.get('BALANCER_MAX_CONCURRENCY', '').strip()
    env_limit = 0
    if env_limit_raw:
        try:
            env_limit = int(env_limit_raw)
        except ValueError:
            print(f"âš ï¸ æ— æ³•è§£æ BALANCER_MAX_CONCURRENCY={env_limit_raw!r}ï¼Œå¿½ç•¥è¯¥é™åˆ¶ã€‚")
            env_limit = 0

    if env_limit > 0:
        MAX_CONCURRENCY = max(1, min(env_limit, total_workers))
    else:
        MAX_CONCURRENCY = total_workers


    INITIAL_DISPATCH_INTERVAL = 1.0
    SECOND_STAGE_INTERVAL = 0.5
    NORMAL_DISPATCH_INTERVAL = 0.2
    MAX_RETRIES = 6
    RATE_LIMIT_MAX_RETRIES = 10
    TIMEOUT_MAX_RETRIES = 6

    # é¢„çƒ­é˜¶æ®µä»»åŠ¡æ•°é‡æ ¹æ®å¹¶å‘å’Œæ€»ä»»åŠ¡è‡ªé€‚åº”
    WARMUP_COUNT = 0
    SECOND_STAGE_COUNT = 0

    # è‡ªé€‚åº”èŠ‚æµå‚æ•°
    ADAPTIVE_WINDOW = 20
    FAILURE_RATE_THRESHOLD = 0.2
    RECOVERY_RATE_THRESHOLD = 0.1
    ADAPTIVE_FAIL_INTERVAL = 0.5
    ADAPTIVE_INCREASE_STEP = 0.1
    ADAPTIVE_DECREASE_STEP = 0.05
    ADAPTIVE_MAX_INTERVAL = 1.5
    MIN_SAMPLE_SIZE = 5

    files_to_process = specific_files or list(batch_info['files'].keys())
    total_tasks_count = len(files_to_process)

    warmup_primary = max(10, MAX_CONCURRENCY * 2)
    warmup_secondary = max(10, MAX_CONCURRENCY)
    WARMUP_COUNT = min(total_tasks_count, warmup_primary)
    SECOND_STAGE_COUNT = max(0, min(total_tasks_count - WARMUP_COUNT, warmup_secondary))

    if env_limit > 0:
        concurrency_source = f"ç¯å¢ƒé™åˆ¶ {env_limit}"
    else:
        concurrency_source = f"å¯ç”¨èŠ‚ç‚¹ {total_workers}"

    print("ğŸš€ å¯åŠ¨ç²¾ç»†åŒ–è°ƒåº¦å®˜ (V5.1):")
    print(f"  ğŸ¯ å…¨å±€å¹¶å‘ä¸Šé™: {MAX_CONCURRENCY} ({concurrency_source})")
    print(f"  â±ï¸ é¢„çƒ­/æ­£å¸¸é—´éš”: {INITIAL_DISPATCH_INTERVAL}s / {NORMAL_DISPATCH_INTERVAL}s")
    print(f"  ğŸ”„ æ¬¡çº§é¢„çƒ­é—´éš”: å‰{WARMUP_COUNT}ä¸ª -> {INITIAL_DISPATCH_INTERVAL}s, åç»­{SECOND_STAGE_COUNT}ä¸ª -> {SECOND_STAGE_INTERVAL}s")

    # --- 2. åˆå§‹åŒ–é˜Ÿåˆ—å’Œæ§åˆ¶å™¨ ---
    task_queue = asyncio.Queue()
    for file_id in files_to_process:
        task_queue.put_nowait((file_id, 0))

    worker_queue = asyncio.Queue()
    for i in range(len(api_servers)):
        worker_queue.put_nowait(i)

    concurrency_semaphore = asyncio.Semaphore(MAX_CONCURRENCY)

    batch_info['server_statuses'] = {}
    for i, server in enumerate(api_servers):
        batch_info['server_statuses'][i] = {
            'name': server.get('name', f'Server-{i}'),
            'status': 'idle',
            'load': 0,
            'max_load': 1,
            'completed_tasks': 0,
            'timeout_tasks': 0,
            'failed_tasks': 0,
            'total_time': 0.0,
        }

    completion_event = asyncio.Event()
    finished_files = set()

    recent_results = deque(maxlen=ADAPTIVE_WINDOW)
    adaptive_interval = NORMAL_DISPATCH_INTERVAL
    metrics_lock = asyncio.Lock()
    rate_limit_counters = defaultdict(int)
    timeout_counters = defaultdict(int)

    async def update_rate_metrics(success: bool):
        nonlocal adaptive_interval
        async with metrics_lock:
            recent_results.append(1 if success else 0)
            if len(recent_results) < MIN_SAMPLE_SIZE:
                return

            success_ratio = sum(recent_results) / len(recent_results)
            failure_rate = 1 - success_ratio

            if failure_rate >= FAILURE_RATE_THRESHOLD:
                new_interval = min(
                    ADAPTIVE_MAX_INTERVAL,
                    max(adaptive_interval, ADAPTIVE_FAIL_INTERVAL) + ADAPTIVE_INCREASE_STEP,
                )
                if new_interval > adaptive_interval:
                    print(f"âš ï¸ æœ€è¿‘å¤±è´¥ç‡ {failure_rate:.0%}ï¼Œæ´¾å‘é—´éš”æå‡è‡³ {new_interval:.2f}s")
                adaptive_interval = new_interval
            elif adaptive_interval > NORMAL_DISPATCH_INTERVAL and failure_rate <= RECOVERY_RATE_THRESHOLD:
                new_interval = max(NORMAL_DISPATCH_INTERVAL, adaptive_interval - ADAPTIVE_DECREASE_STEP)
                if new_interval < adaptive_interval:
                    print(f"âœ… å¤±è´¥ç‡å›è½è‡³ {failure_rate:.0%}ï¼Œæ´¾å‘é—´éš”å›è°ƒè‡³ {new_interval:.2f}s")
                adaptive_interval = new_interval

    batch_info['completed_files'] = 0

    async def worker(worker_id, file_id, retry_count):
        server_info = api_servers[worker_id]
        server_name = server_info.get('name', f"Server-{worker_id}")
        server_url = server_info.get('url')
        api_key = server_info.get('apiKey', server_info.get('api_key', ''))

        success = False
        skip_metrics = False

        try:
            if batch_id not in batch_status or file_id not in batch_status[batch_id]['files']:
                skip_metrics = True
                return

            filename = batch_info['files'][file_id]['filename']
            batch_info['files'][file_id]['status'] = 'processing'
            batch_info['files'][file_id]['stage'] = f'å¤„ç†ä¸­ @{server_name}'
            batch_info['server_statuses'][worker_id]['status'] = 'busy'
            batch_info['server_statuses'][worker_id]['load'] = 1

            input_path = os.path.join(batch_upload_dir, filename)
            output_path = os.path.join(batch_upload_dir, filename.replace('.md', '.mp3'))
            with open(input_path, 'r', encoding='utf-8') as f:
                text = f.read()

            await asyncio.sleep(random.uniform(0.0, 0.05))

            start_time = time.time()
            status_code = None
            error_detail = None
            async with aiohttp.ClientSession() as session:
                if global_api_semaphore is not None:
                    async with global_api_semaphore:
                        success, status_code, error_detail = await async_text_to_speech(
                            session, text, output_path, voice, speed, server_url, api_key, timeout_seconds=300
                        )
                else:
                    success, status_code, error_detail = await async_text_to_speech(
                        session, text, output_path, voice, speed, server_url, api_key, timeout_seconds=300
                    )

            error_text = (error_detail or "").lower()
            is_timeout = (error_detail == 'timeout') or ('timeout' in error_text)
            is_rate_limited = (
                status_code in {429, 503}
                or 'too many requests' in error_text
                or 'too many subrequests' in error_text
                or 'rate limit' in error_text
            )
            if status_code == 500 and 'too many' in error_text:
                is_rate_limited = True

            cost = time.time() - start_time
            batch_info['server_statuses'][worker_id]['total_time'] += cost

            if success:
                rate_limit_counters.pop(file_id, None)
                timeout_counters.pop(file_id, None)
                batch_info['files'][file_id]['status'] = 'completed'
                batch_info['files'][file_id]['stage'] = 'âœ… å®Œæˆ'
                if file_id not in finished_files:
                    finished_files.add(file_id)
                    batch_info['completed_files'] += 1
                    batch_info['current_file'] = batch_info['completed_files']
                batch_info['server_statuses'][worker_id]['completed_tasks'] += 1
                print(f"âœ… ä»»åŠ¡å®Œæˆ: {filename} (æœåŠ¡å™¨: {server_name}, è€—æ—¶: {cost:.2f}ç§’)")
            else:
                batch_info['server_statuses'][worker_id]['status'] = 'error'
                if is_rate_limited:
                    rate_limit_counters[file_id] += 1
                    rate_limit_attempt = rate_limit_counters[file_id]
                    if rate_limit_attempt > RATE_LIMIT_MAX_RETRIES:
                        stage_msg = f'âŒ é™æµå¤±è´¥ (å·²é‡è¯•{RATE_LIMIT_MAX_RETRIES}æ¬¡)'
                        batch_info['files'][file_id]['status'] = 'failed'
                        batch_info['files'][file_id]['stage'] = stage_msg
                        rate_limit_counters.pop(file_id, None)
                        timeout_counters.pop(file_id, None)
                        batch_info['server_statuses'][worker_id]['failed_tasks'] += 1
                        if file_id not in finished_files:
                            finished_files.add(file_id)
                            batch_info['completed_files'] += 1
                            batch_info['current_file'] = batch_info['completed_files']
                        print(
                            f"âŒ é™æµé‡è¯•è€—å°½: {filename} (æœåŠ¡å™¨: {server_name}, çŠ¶æ€ç : {status_code}, è€—æ—¶: {cost:.2f}ç§’)"
                        )
                    else:
                        delay_exponent = min(6, rate_limit_attempt + 1)
                        delay = (2 ** delay_exponent) + random.uniform(0, 2.0)
                        print(
                            f"ğŸ›‘ é™æµ {status_code}: {filename} @ {server_name}ï¼Œç¬¬{rate_limit_attempt}æ¬¡ç­‰å¾…ï¼Œ{delay:.2f}s åé‡è¯•"
                        )

                        async def requeue_rate_limit(delay_s: float, item):
                            await asyncio.sleep(delay_s)
                            await task_queue.put(item)

                        asyncio.create_task(requeue_rate_limit(delay, (file_id, retry_count)))
                        batch_info['files'][file_id]['stage'] = (
                            f'ç­‰å¾…é™æµæ¢å¤ ({rate_limit_attempt}/{RATE_LIMIT_MAX_RETRIES})'
                        )
                elif is_timeout:
                    batch_info['server_statuses'][worker_id]['timeout_tasks'] += 1
                    timeout_counters[file_id] += 1
                    timeout_attempt = timeout_counters[file_id]
                    if timeout_attempt > TIMEOUT_MAX_RETRIES:
                        batch_info['files'][file_id]['status'] = 'failed'
                        batch_info['files'][file_id]['stage'] = (
                            f'âŒ è¶…æ—¶è¶…å‡ºä¸Šé™ ({TIMEOUT_MAX_RETRIES}æ¬¡)'
                        )
                        rate_limit_counters.pop(file_id, None)
                        timeout_counters.pop(file_id, None)
                        batch_info['server_statuses'][worker_id]['failed_tasks'] += 1
                        if file_id not in finished_files:
                            finished_files.add(file_id)
                            batch_info['completed_files'] += 1
                            batch_info['current_file'] = batch_info['completed_files']
                        print(
                            f"âŒ è¶…æ—¶é‡è¯•è€—å°½: {filename} (æœåŠ¡å™¨: {server_name}, è€—æ—¶: {cost:.2f}ç§’)"
                        )
                    else:
                        delay = 5.0 * timeout_attempt + random.uniform(0, 3.0)
                        print(
                            f"â³ è¶…æ—¶é‡è¯•: {filename} @ {server_name} ç¬¬{timeout_attempt}æ¬¡ï¼Œå°†åœ¨ {delay:.2f}s åé‡è¯•"
                        )

                        async def requeue_timeout(delay_s: float, item):
                            await asyncio.sleep(delay_s)
                            await task_queue.put(item)

                        asyncio.create_task(requeue_timeout(delay, (file_id, retry_count)))
                        batch_info['files'][file_id]['stage'] = (
                            f'ç­‰å¾…è¶…æ—¶æ¢å¤ ({timeout_attempt}/{TIMEOUT_MAX_RETRIES})'
                        )
                elif retry_count < MAX_RETRIES:
                    batch_info['server_statuses'][worker_id]['failed_tasks'] += 1
                    delay = (2 ** (retry_count + 1)) + random.uniform(0, 2.0)
                    print(
                        f"âŒ ä»»åŠ¡å¤±è´¥: {filename} (æœåŠ¡å™¨: {server_name}, çŠ¶æ€ç : {status_code}, è€—æ—¶: {cost:.2f}ç§’)ï¼Œå°†åœ¨ {delay:.2f}s åé‡è¯•"
                    )

                    async def requeue_general(delay_s: float, item):
                        await asyncio.sleep(delay_s)
                        await task_queue.put(item)

                    asyncio.create_task(requeue_general(delay, (file_id, retry_count + 1)))
                    batch_info['files'][file_id]['stage'] = f'ç­‰å¾…é‡è¯• ({retry_count+1}/{MAX_RETRIES})'
                else:
                    rate_limit_counters.pop(file_id, None)
                    timeout_counters.pop(file_id, None)
                    batch_info['server_statuses'][worker_id]['failed_tasks'] += 1
                    batch_info['files'][file_id]['status'] = 'failed'
                    batch_info['files'][file_id]['stage'] = 'âŒ å¤±è´¥ (å·²è¾¾ä¸Šé™)'
                    if file_id not in finished_files:
                        finished_files.add(file_id)
                        batch_info['completed_files'] += 1
                        batch_info['current_file'] = batch_info['completed_files']
        except Exception as e:
            print(f"ğŸ’¥ å·¥äºº {server_name} å¼‚å¸¸: {file_id} -> {e}")
            batch_info['server_statuses'][worker_id]['status'] = 'error'
            batch_info['server_statuses'][worker_id]['failed_tasks'] += 1
            if retry_count < MAX_RETRIES:
                delay = (2 ** (retry_count + 1)) + random.uniform(0, 2.0)

                async def requeue_exception(delay_s: float, item):
                    await asyncio.sleep(delay_s)
                    await task_queue.put(item)

                asyncio.create_task(requeue_exception(delay, (file_id, retry_count + 1)))
                batch_info['files'][file_id]['stage'] = f'ç­‰å¾…é‡è¯• ({retry_count+1}/{MAX_RETRIES})'
            else:
                if batch_id in batch_status and file_id in batch_status[batch_id]['files']:
                    rate_limit_counters.pop(file_id, None)
                    timeout_counters.pop(file_id, None)
                    batch_info['files'][file_id]['status'] = 'failed'
                    batch_info['files'][file_id]['stage'] = 'ğŸ’¥ å¤„ç†å¼‚å¸¸'
                    if file_id not in finished_files:
                        finished_files.add(file_id)
                        batch_info['completed_files'] += 1
                        batch_info['current_file'] = batch_info['completed_files']
        finally:
            if not skip_metrics:
                await update_rate_metrics(success)

            batch_info['server_statuses'][worker_id]['load'] = 0
            batch_info['server_statuses'][worker_id]['status'] = 'idle'

            await worker_queue.put(worker_id)
            concurrency_semaphore.release()

            if len(finished_files) >= total_tasks_count:
                completion_event.set()

    async def dispatcher():
        dispatched_count = 0
        try:
            while not completion_event.is_set():
                await concurrency_semaphore.acquire()

                if completion_event.is_set():
                    concurrency_semaphore.release()
                    break

                try:
                    worker_id = await worker_queue.get()
                except asyncio.CancelledError:
                    concurrency_semaphore.release()
                    raise

                if completion_event.is_set():
                    await worker_queue.put(worker_id)
                    concurrency_semaphore.release()
                    break

                try:
                    file_id, retry_count = task_queue.get_nowait()
                except asyncio.QueueEmpty:
                    await worker_queue.put(worker_id)
                    concurrency_semaphore.release()
                    if completion_event.is_set():
                        break
                    await asyncio.sleep(0.1)
                    continue

                asyncio.create_task(worker(worker_id, file_id, retry_count))
                task_queue.task_done()
                dispatched_count += 1

                remaining = task_queue.qsize()
                idle_workers = worker_queue.qsize()

                if dispatched_count <= WARMUP_COUNT:
                    base_interval = INITIAL_DISPATCH_INTERVAL
                elif dispatched_count <= WARMUP_COUNT + SECOND_STAGE_COUNT:
                    base_interval = SECOND_STAGE_INTERVAL
                else:
                    base_interval = NORMAL_DISPATCH_INTERVAL

                interval = max(base_interval, adaptive_interval)

                print(
                    f"ğŸ§­ æ´¾å‘ä»»åŠ¡: {dispatched_count} | å‰©ä½™é˜Ÿåˆ—: {remaining} | ç©ºé—²æœåŠ¡å™¨: {idle_workers} | å½“å‰é—´éš”: {interval:.2f}s"
                )

                if interval > 0:
                    await asyncio.sleep(interval)
        except asyncio.CancelledError:
            pass

    dispatcher_task = asyncio.create_task(dispatcher())
    await completion_event.wait()
    dispatcher_task.cancel()
    with contextlib.suppress(asyncio.CancelledError):
        await dispatcher_task

    print("ğŸ‰ V5.1 ç²¾ç»†åŒ–è°ƒåº¦å¤„ç†å®Œæˆï¼")

async def process_single_file_with_callback(session, batch_id, batch_upload_dir, voice, speed, api_servers, file_id, server_id, server_stats, concurrency, callback):
    """å¼‚æ­¥å¤„ç†å•ä¸ªæ–‡ä»¶ï¼Œå¸¦å›è°ƒæœºåˆ¶"""
    start_time = time.time()
    
    try:
        batch_info = batch_status[batch_id]
        file_info = batch_info['files'][file_id]
        filename = file_info['filename']
        selected_server = api_servers[server_id]
        
        # æ›´æ–°æ–‡ä»¶çŠ¶æ€
        file_info['status'] = 'processing'
        file_info['progress'] = 10
        file_info['stage'] = 'ğŸ“– è¯»å–æ–‡ä»¶...'
        
        # è¯»å–å®Œæ•´çš„Markdownæ–‡ä»¶å†…å®¹
        md_path = os.path.join(batch_upload_dir, filename)
        
        with open(md_path, 'r', encoding='utf-8') as f:
            full_text = f.read()
        
        file_info['progress'] = 20
        file_info['stage'] = 'ğŸ“¤ å‡†å¤‡å‘é€åˆ°TTS API...'
        
        # ç”ŸæˆMP3æ–‡ä»¶è·¯å¾„
        mp3_filename = os.path.splitext(filename)[0] + '.mp3'
        mp3_path = os.path.join(batch_upload_dir, mp3_filename)
        
        file_info['progress'] = 30
        file_info['stage'] = f'ğŸµ æ­£åœ¨è½¬æ¢ (æœåŠ¡å™¨: {selected_server["name"]})...'
        
        # è°ƒç”¨å¼‚æ­¥TTSè½¬æ¢
        api_key = selected_server.get('apiKey', selected_server.get('api_key', ''))
        success, _, _ = await async_text_to_speech(
            session, full_text, mp3_path, voice, speed, 
            selected_server['url'], api_key
        )
        
        end_time = time.time()
        processing_time = end_time - start_time
        
        if success:
            file_info['status'] = 'completed'
            file_info['progress'] = 100
            file_info['stage'] = 'âœ… è½¬æ¢å®Œæˆ'
            print(f"âœ… {filename} è½¬æ¢æˆåŠŸ (æœåŠ¡å™¨: {selected_server['name']}, è€—æ—¶: {processing_time:.2f}ç§’)")
        else:
            file_info['status'] = 'failed'
            file_info['progress'] = 100
            file_info['stage'] = f'âŒ è½¬æ¢å¤±è´¥ (æœåŠ¡å™¨: {selected_server["name"]})'
            print(f"âŒ {filename} è½¬æ¢å¤±è´¥ (æœåŠ¡å™¨: {selected_server['name']}, è€—æ—¶: {processing_time:.2f}ç§’)")
        
        # è°ƒç”¨å›è°ƒå‡½æ•°
        await callback(file_id, server_id, success, processing_time)
        return success
            
    except Exception as e:
        end_time = time.time()
        processing_time = end_time - start_time
        
        print(f"ğŸ’¥ å¤„ç†æ–‡ä»¶ {file_id} æ—¶å‡ºé”™: {str(e)}", file=sys.stderr)
        if batch_id in batch_status:
            batch_info = batch_status[batch_id]
            if file_id in batch_info['files']:
                file_info = batch_info['files'][file_id]
                file_info['status'] = 'failed'
                file_info['progress'] = 100
                file_info['stage'] = f'ğŸ’¥ å¤„ç†å¼‚å¸¸: {str(e)}'
        
        # è°ƒç”¨å›è°ƒå‡½æ•°
        await callback(file_id, server_id, False, processing_time)
        return False

async def process_single_file_with_server_tracking(session, batch_id, batch_upload_dir, voice, speed, api_servers, file_id, server_id, server_stats, concurrency):
    """å¼‚æ­¥å¤„ç†å•ä¸ªæ–‡ä»¶ï¼Œå¸¦æœåŠ¡å™¨çŠ¶æ€è·Ÿè¸ª"""
    start_time = time.time()
    
    try:
        batch_info = batch_status[batch_id]
        file_info = batch_info['files'][file_id]
        filename = file_info['filename']
        selected_server = api_servers[server_id]
        
        # æ›´æ–°æ–‡ä»¶çŠ¶æ€
        file_info['status'] = 'processing'
        file_info['progress'] = 10
        file_info['stage'] = 'ğŸ“– è¯»å–æ–‡ä»¶...'
        
        # è¯»å–å®Œæ•´çš„Markdownæ–‡ä»¶å†…å®¹
        md_path = os.path.join(batch_upload_dir, filename)
        
        with open(md_path, 'r', encoding='utf-8') as f:
            full_text = f.read()
        
        file_info['progress'] = 20
        file_info['stage'] = 'ğŸ“¤ å‡†å¤‡å‘é€åˆ°TTS API...'
        
        # ç”ŸæˆMP3æ–‡ä»¶è·¯å¾„
        mp3_filename = os.path.splitext(filename)[0] + '.mp3'
        mp3_path = os.path.join(batch_upload_dir, mp3_filename)
        
        file_info['progress'] = 30
        file_info['stage'] = f'ğŸµ æ­£åœ¨è½¬æ¢ (æœåŠ¡å™¨: {selected_server["name"]})...'
        
        # è°ƒç”¨å¼‚æ­¥TTSè½¬æ¢
        api_key = selected_server.get('apiKey', selected_server.get('api_key', ''))
        success, _, _ = await async_text_to_speech(
            session, full_text, mp3_path, voice, speed, 
            selected_server['url'], api_key
        )
        
        # æ›´æ–°æœåŠ¡å™¨ç»Ÿè®¡
        end_time = time.time()
        processing_time = end_time - start_time
        server_stats[server_id]['active_tasks'] -= 1
        server_stats[server_id]['completed_tasks'] += 1
        server_stats[server_id]['total_time'] += processing_time
        
        # æ›´æ–°æœåŠ¡å™¨çŠ¶æ€
        if batch_id in batch_status:
            batch_info = batch_status[batch_id]
            current_load = server_stats[server_id]['active_tasks']
            batch_info['server_statuses'][server_id]['load'] = current_load
            batch_info['server_statuses'][server_id]['completed_tasks'] = server_stats[server_id]['completed_tasks']
            batch_info['server_statuses'][server_id]['total_time'] = server_stats[server_id]['total_time']
            
            if current_load == 0:
                batch_info['server_statuses'][server_id]['status'] = 'idle'
            elif current_load >= concurrency:
                batch_info['server_statuses'][server_id]['status'] = 'full'
            else:
                batch_info['server_statuses'][server_id]['status'] = 'busy'
        
        if success:
            file_info['status'] = 'completed'
            file_info['progress'] = 100
            file_info['stage'] = 'âœ… è½¬æ¢å®Œæˆ'
            print(f"âœ… {filename} è½¬æ¢æˆåŠŸ (æœåŠ¡å™¨: {selected_server['name']}, è€—æ—¶: {processing_time:.2f}ç§’)")
            return True
        else:
            file_info['status'] = 'failed'
            file_info['progress'] = 100
            file_info['stage'] = f'âŒ è½¬æ¢å¤±è´¥ (æœåŠ¡å™¨: {selected_server["name"]})'
            print(f"âŒ {filename} è½¬æ¢å¤±è´¥ (æœåŠ¡å™¨: {selected_server['name']}, è€—æ—¶: {processing_time:.2f}ç§’)")
            return False
            
    except Exception as e:
        # æ›´æ–°æœåŠ¡å™¨ç»Ÿè®¡
        end_time = time.time()
        processing_time = end_time - start_time
        server_stats[server_id]['active_tasks'] -= 1
        server_stats[server_id]['completed_tasks'] += 1
        server_stats[server_id]['total_time'] += processing_time
        
        # æ›´æ–°æœåŠ¡å™¨çŠ¶æ€
        if batch_id in batch_status:
            batch_info = batch_status[batch_id]
            current_load = server_stats[server_id]['active_tasks']
            batch_info['server_statuses'][server_id]['load'] = current_load
            batch_info['server_statuses'][server_id]['completed_tasks'] = server_stats[server_id]['completed_tasks']
            batch_info['server_statuses'][server_id]['total_time'] = server_stats[server_id]['total_time']
            
            if current_load == 0:
                batch_info['server_statuses'][server_id]['status'] = 'idle'
            elif current_load >= concurrency:
                batch_info['server_statuses'][server_id]['status'] = 'full'
            else:
                batch_info['server_statuses'][server_id]['status'] = 'busy'
        
        print(f"ğŸ’¥ å¤„ç†æ–‡ä»¶ {file_id} æ—¶å‡ºé”™: {str(e)}", file=sys.stderr)
        if batch_id in batch_status:
            batch_info = batch_status[batch_id]
            if file_id in batch_info['files']:
                file_info = batch_info['files'][file_id]
                file_info['status'] = 'failed'
                file_info['progress'] = 100
                file_info['stage'] = f'ğŸ’¥ å¤„ç†å¼‚å¸¸: {str(e)}'
        return False

async def process_single_file_async(session, semaphore, batch_id, batch_upload_dir, voice, speed, api_servers, file_id):
    """å¼‚æ­¥å¤„ç†å•ä¸ªæ–‡ä»¶"""
    async with semaphore:  # æ§åˆ¶å¹¶å‘æ•°
        try:
            batch_info = batch_status[batch_id]
            file_info = batch_info['files'][file_id]
            filename = file_info['filename']
            
            # æ›´æ–°æ–‡ä»¶çŠ¶æ€
            file_info['status'] = 'processing'
            file_info['progress'] = 10
            file_info['stage'] = 'ğŸ“– è¯»å–æ–‡ä»¶...'
            
            # è¯»å–å®Œæ•´çš„Markdownæ–‡ä»¶å†…å®¹
            md_path = os.path.join(batch_upload_dir, filename)
            
            with open(md_path, 'r', encoding='utf-8') as f:
                full_text = f.read()
            
            file_info['progress'] = 20
            file_info['stage'] = 'ğŸ“¤ å‡†å¤‡å‘é€åˆ°TTS API...'
            
            # ç”ŸæˆMP3æ–‡ä»¶è·¯å¾„
            mp3_filename = os.path.splitext(filename)[0] + '.mp3'
            mp3_path = os.path.join(batch_upload_dir, mp3_filename)
            
            file_info['progress'] = 30
            file_info['stage'] = 'â³ ç­‰å¾…TTSå¤„ç†ä¸­...'
            
            # è´Ÿè½½å‡è¡¡ï¼šè½®è¯¢é€‰æ‹©æœåŠ¡å™¨
            server_index = hash(file_id) % len(api_servers)  # ä½¿ç”¨å“ˆå¸Œç¡®ä¿ä¸€è‡´æ€§
            selected_server = api_servers[server_index]
            
            server_name = selected_server.get('name', 'Unknown')
            api_url = selected_server.get('url', '')
            api_key = selected_server.get('apiKey', '')
            
            # è°ƒè¯•æ—¥å¿—ï¼šæ˜¾ç¤ºæœåŠ¡å™¨åˆ†é…
            print(f"ğŸ”„ æ–‡ä»¶ {filename} åˆ†é…ç»™æœåŠ¡å™¨: {server_name} ({api_url})")
            
            file_info['stage'] = f'â³ ä½¿ç”¨æœåŠ¡å™¨ {server_name} å¤„ç†ä¸­...'
            
            # å¼‚æ­¥è°ƒç”¨TTSè½¬æ¢
            success, _, _ = await async_text_to_speech(session, full_text, mp3_path, voice, speed, api_url, api_key)
            
            if success:
                file_info['progress'] = 90
                file_info['stage'] = 'ğŸ’¾ ä¿å­˜éŸ³é¢‘æ–‡ä»¶...'
                
                file_info['status'] = 'completed'
                file_info['progress'] = 100
                file_info['stage'] = 'âœ… è½¬æ¢å®Œæˆ'
                print(f"âœ… {filename} è½¬æ¢æˆåŠŸ (æœåŠ¡å™¨: {server_name})")
            else:
                file_info['status'] = 'failed'
                file_info['progress'] = 100
                file_info['stage'] = f'âŒ è½¬æ¢å¤±è´¥ (æœåŠ¡å™¨: {server_name})'
                print(f"âŒ {filename} è½¬æ¢å¤±è´¥ (æœåŠ¡å™¨: {server_name})")
            
            # æ›´æ–°å®Œæˆè®¡æ•°
            batch_info['completed_files'] += 1
            batch_info['current_file'] = batch_info['completed_files']
            
            return success
            
        except Exception as e:
            file_info['status'] = 'failed'
            file_info['progress'] = 100
            file_info['stage'] = f'âŒ å¤„ç†å¼‚å¸¸: {str(e)}'
            print(f"âŒ {filename} å¤„ç†å¼‚å¸¸: {str(e)}")
            batch_info['completed_files'] += 1
            batch_info['current_file'] = batch_info['completed_files']
            return False

def process_files_with_load_balancing(batch_id, batch_upload_dir, voice, speed, api_servers, concurrency):
    """ä½¿ç”¨è´Ÿè½½å‡è¡¡å’Œå¹¶å‘å¤„ç†æ–‡ä»¶"""
    if batch_id not in batch_status:
        return
    
    batch_info = batch_status[batch_id]
    files_to_process = list(batch_info['files'].keys())
    
    # è°ƒè¯•æ—¥å¿—ï¼šæ˜¾ç¤ºè´Ÿè½½å‡è¡¡é…ç½®
    print(f"ğŸš€ å¼€å§‹è´Ÿè½½å‡è¡¡å¤„ç†:")
    print(f"  ğŸ“ æ‰¹æ¬¡ID: {batch_id}")
    print(f"  ğŸ“„ æ–‡ä»¶æ•°é‡: {len(files_to_process)}")
    print(f"  ğŸ–¥ï¸ å¯ç”¨æœåŠ¡å™¨: {len(api_servers)}")
    print(f"  âš¡ å¹¶å‘åº¦: {concurrency}")
    
    # åˆ›å»ºæœåŠ¡å™¨è½®è¯¢ç´¢å¼•
    server_index = 0
    
    # ä½¿ç”¨çº¿ç¨‹æ± è¿›è¡Œå¹¶å‘å¤„ç†
    import threading
    from concurrent.futures import ThreadPoolExecutor, as_completed
    import queue
    
    # åˆ›å»ºä»»åŠ¡é˜Ÿåˆ—
    task_queue = queue.Queue()
    for file_id in files_to_process:
        task_queue.put(file_id)
    
    # åˆ›å»ºçº¿ç¨‹æ± 
    max_workers = min(concurrency, len(files_to_process), len(api_servers))
    
    def process_single_file(file_id):
        """å¤„ç†å•ä¸ªæ–‡ä»¶"""
        try:
            file_info = batch_info['files'][file_id]
            filename = file_info['filename']
            
            # æ›´æ–°æ–‡ä»¶çŠ¶æ€
            file_info['status'] = 'processing'
            file_info['progress'] = 10
            file_info['stage'] = 'ğŸ“– è¯»å–æ–‡ä»¶...'
            
            # è¯»å–å®Œæ•´çš„Markdownæ–‡ä»¶å†…å®¹
            md_path = os.path.join(batch_upload_dir, filename)
            
            with open(md_path, 'r', encoding='utf-8') as f:
                full_text = f.read()
            
            file_info['progress'] = 20
            file_info['stage'] = 'ğŸ“¤ å‡†å¤‡å‘é€åˆ°TTS API...'
            
            # ç”ŸæˆMP3æ–‡ä»¶è·¯å¾„
            mp3_filename = os.path.splitext(filename)[0] + '.mp3'
            mp3_path = os.path.join(batch_upload_dir, mp3_filename)
            
            file_info['progress'] = 30
            file_info['stage'] = 'â³ ç­‰å¾…TTSå¤„ç†ä¸­...'
            
            # è´Ÿè½½å‡è¡¡ï¼šè½®è¯¢é€‰æ‹©æœåŠ¡å™¨
            nonlocal server_index
            selected_server = api_servers[server_index % len(api_servers)]
            server_index += 1
            
            server_name = selected_server.get('name', 'Unknown')
            api_url = selected_server.get('url', '')
            api_key = selected_server.get('apiKey', '')
            
            # è°ƒè¯•æ—¥å¿—ï¼šæ˜¾ç¤ºæœåŠ¡å™¨åˆ†é…
            print(f"ğŸ”„ æ–‡ä»¶ {filename} åˆ†é…ç»™æœåŠ¡å™¨: {server_name} ({api_url})")
            
            file_info['stage'] = f'â³ ä½¿ç”¨æœåŠ¡å™¨ {server_name} å¤„ç†ä¸­...'
            
            # è°ƒç”¨TTSè½¬æ¢
            success = text_to_speech(full_text, mp3_path, voice, speed, api_url, api_key)
            
            if success:
                file_info['progress'] = 90
                file_info['stage'] = 'ğŸ’¾ ä¿å­˜éŸ³é¢‘æ–‡ä»¶...'
                
                file_info['status'] = 'completed'
                file_info['progress'] = 100
                file_info['stage'] = 'âœ… è½¬æ¢å®Œæˆ'
                print(f"âœ… {filename} è½¬æ¢æˆåŠŸ (æœåŠ¡å™¨: {server_name})")
            else:
                file_info['status'] = 'failed'
                file_info['progress'] = 100
                file_info['stage'] = f'âŒ è½¬æ¢å¤±è´¥ (æœåŠ¡å™¨: {server_name})'
                print(f"âŒ {filename} è½¬æ¢å¤±è´¥ (æœåŠ¡å™¨: {server_name})")
            
            return file_id, success
            
        except Exception as e:
            file_info['status'] = 'failed'
            file_info['progress'] = 100
            file_info['stage'] = f'âŒ å¤„ç†å¼‚å¸¸: {str(e)}'
            print(f"âŒ {filename} å¤„ç†å¼‚å¸¸: {str(e)}")
            return file_id, False
    
    # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œä»»åŠ¡
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        future_to_file = {
            executor.submit(process_single_file, file_id): file_id 
            for file_id in files_to_process
        }
        
        # å¤„ç†å®Œæˆçš„ä»»åŠ¡
        for future in as_completed(future_to_file):
            file_id, success = future.result()
            batch_info['completed_files'] += 1
            
            # æ›´æ–°å½“å‰å¤„ç†æ–‡ä»¶è®¡æ•°
            batch_info['current_file'] = batch_info['completed_files']
    
    print(f"ğŸ‰ æ‰¹é‡å¤„ç†å®Œæˆ: {batch_info['completed_files']}/{batch_info['total_files']} ä¸ªæ–‡ä»¶")
    print(f"ğŸ“Š ä½¿ç”¨äº† {len(api_servers)} ä¸ªæœåŠ¡å™¨ï¼Œå¹¶å‘åº¦: {max_workers}")

@app.route('/progress/<batch_id>')
def get_progress(batch_id):
    """è·å–æ‰¹é‡å¤„ç†è¿›åº¦"""
    if batch_id not in batch_status:
        return jsonify({'error': 'æ‰¹æ¬¡ä¸å­˜åœ¨'}), 404
    
    status = batch_status[batch_id]
    return jsonify({
        'batch_id': batch_id,
        'total_files': status['total_files'],
        'completed_files': status['completed_files'],
        'current_file': status.get('current_file', 0),
        'files': status['files']
    })

@app.route('/server_status/<batch_id>')
def get_server_status(batch_id):
    """è·å–æœåŠ¡å™¨çŠ¶æ€ä¿¡æ¯"""
    if batch_id not in batch_status:
        return jsonify({'error': 'æ‰¹æ¬¡ä¸å­˜åœ¨'}), 404
    
    # ä»batch_statusä¸­è·å–æœåŠ¡å™¨çŠ¶æ€ä¿¡æ¯
    status = batch_status[batch_id]
    server_statuses = status.get('server_statuses', {})
    
    return jsonify({
        'batch_id': batch_id,
        'server_statuses': server_statuses,
        'timestamp': time.time()
    })

@app.route('/retry_failed', methods=['POST'])
def retry_failed_files():
    """é‡è¯•å¤±è´¥çš„æ–‡ä»¶"""
    try:
        batch_id = request.form.get('batch_id')
        api_servers_json = request.form.get('api_servers')
        concurrency = int(request.form.get('concurrency', 2))
        voice = request.form.get('voice', 'zh-CN-XiaoxiaoNeural')
        speed = float(request.form.get('speed', 1.0))
        
        if not batch_id or batch_id not in batch_status:
            return jsonify({'error': 'æ‰¹æ¬¡ä¸å­˜åœ¨'}), 404
        
        # è§£æAPIæœåŠ¡å™¨åˆ—è¡¨
        try:
            api_servers = json.loads(api_servers_json) if api_servers_json else []
        except json.JSONDecodeError:
            return jsonify({'error': 'APIæœåŠ¡å™¨é…ç½®æ ¼å¼é”™è¯¯'}), 400
        
        # è¿‡æ»¤å¯ç”¨çš„æœåŠ¡å™¨
        enabled_servers = [server for server in api_servers if server.get('enabled', False)]
        if not enabled_servers:
            return jsonify({'error': 'æ²¡æœ‰å¯ç”¨çš„APIæœåŠ¡å™¨'}), 400
        
        batch_info = batch_status[batch_id]
        
        # æ‰¾å‡ºå¤±è´¥çš„æ–‡ä»¶
        failed_files = []
        for file_id, file_info in batch_info['files'].items():
            if file_info['status'] == 'failed':
                failed_files.append(file_id)
        
        if not failed_files:
            return jsonify({'error': 'æ²¡æœ‰å¤±è´¥çš„æ–‡ä»¶éœ€è¦é‡è¯•'}), 400
        
        print(f"ğŸ”„ å¼€å§‹é‡è¯•å¤±è´¥æ–‡ä»¶:")
        print(f"  ğŸ“ æ‰¹æ¬¡ID: {batch_id}")
        print(f"  ğŸ“„ å¤±è´¥æ–‡ä»¶æ•°é‡: {len(failed_files)}")
        print(f"  ğŸ–¥ï¸ å¯ç”¨æœåŠ¡å™¨: {len(enabled_servers)}")
        print(f"  âš¡ å¹¶å‘åº¦: {concurrency}")
        
        # é‡ç½®å¤±è´¥æ–‡ä»¶çš„çŠ¶æ€
        for file_id in failed_files:
            file_info = batch_info['files'][file_id]
            file_info['status'] = 'pending'
            file_info['progress'] = 0
            file_info['stage'] = 'â³ ç­‰å¾…é‡è¯•...'
            file_info['error'] = None
        
        # æ›´æ–°æ‰¹æ¬¡çŠ¶æ€
        batch_info['status'] = 'processing'
        batch_info['completed_files'] = batch_info['total_files'] - len(failed_files)
        batch_info['current_file'] = batch_info['completed_files']
        
        # è·å–æ‰¹æ¬¡ç›®å½•
        batch_upload_dir = batch_info['upload_dir']
        
        # å¯åŠ¨å¼‚æ­¥é‡è¯•å¤„ç†
        retry_thread = threading.Thread(
            target=run_async_processing,
            args=(batch_id, batch_upload_dir, voice, speed, enabled_servers, concurrency, failed_files)
        )
        retry_thread.daemon = True
        retry_thread.start()
        
        return jsonify({
            'success': True,
            'message': f'å¼€å§‹é‡è¯• {len(failed_files)} ä¸ªå¤±è´¥æ–‡ä»¶',
            'retry_files': len(failed_files)
        })
        
    except Exception as e:
        print(f"é‡è¯•å¤±è´¥æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}", file=sys.stderr)
        return jsonify({'error': f'é‡è¯•å¤±è´¥: {str(e)}'}), 500

@app.route('/api/folders')
def get_folders():
    """è·å–uploadsç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶å¤¹åˆ—è¡¨"""
    try:
        upload_dir = app.config['UPLOAD_FOLDER']
        if not os.path.exists(upload_dir):
            return jsonify({'folders': []})
        
        folders = []
        for item in os.listdir(upload_dir):
            item_path = os.path.join(upload_dir, item)
            if os.path.isdir(item_path):
                # è·å–æ–‡ä»¶å¤¹ä¿¡æ¯
                files = os.listdir(item_path)
                md_files = [f for f in files if f.endswith('.md')]
                mp3_files = [f for f in files if f.endswith('.mp3')]
                
                # è·å–æ–‡ä»¶å¤¹åˆ›å»ºæ—¶é—´
                create_time = os.path.getctime(item_path)
                
                folders.append({
                    'name': item,
                    'path': item_path,
                    'md_count': len(md_files),
                    'mp3_count': len(mp3_files),
                    'total_files': len(files),
                    'create_time': time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(create_time))
                })
        
        # æŒ‰åˆ›å»ºæ—¶é—´å€’åºæ’åˆ—
        folders.sort(key=lambda x: x['create_time'], reverse=True)
        return jsonify({'folders': folders})
    
    except Exception as e:
        return jsonify({'error': f'è·å–æ–‡ä»¶å¤¹åˆ—è¡¨å¤±è´¥: {str(e)}'}), 500

@app.route('/api/download/<folder_name>')
def download_folder(folder_name):
    """ä¸‹è½½æŒ‡å®šæ–‡ä»¶å¤¹çš„ZIPåŒ…"""
    try:
        # å®‰å…¨æ£€æŸ¥ï¼šé˜²æ­¢è·¯å¾„éå†æ”»å‡»
        if '..' in folder_name or '/' in folder_name or '\\' in folder_name:
            return jsonify({'error': 'æ— æ•ˆçš„æ–‡ä»¶å¤¹åç§°'}), 400
        
        upload_dir = app.config['UPLOAD_FOLDER']
        folder_path = os.path.join(upload_dir, folder_name)
        
        if not os.path.exists(folder_path) or not os.path.isdir(folder_path):
            return jsonify({'error': 'æ–‡ä»¶å¤¹ä¸å­˜åœ¨'}), 404
        
        # åˆ›å»ºå†…å­˜ä¸­çš„ZIPæ–‡ä»¶
        memory_file = io.BytesIO()
        
        with zipfile.ZipFile(memory_file, 'w', zipfile.ZIP_DEFLATED) as zipf:
            for root, dirs, files in os.walk(folder_path):
                for file in files:
                    file_path = os.path.join(root, file)
                    # è®¡ç®—ç›¸å¯¹è·¯å¾„ï¼Œä¿æŒæ–‡ä»¶å¤¹ç»“æ„
                    arcname = os.path.relpath(file_path, folder_path)
                    zipf.write(file_path, arcname)
        
        memory_file.seek(0)
        
        # ç”Ÿæˆä¸‹è½½æ–‡ä»¶å
        download_filename = f"{folder_name}.zip"
        
        return send_file(
            memory_file,
            as_attachment=True,
            download_name=download_filename,
            mimetype='application/zip'
        )
    
    except Exception as e:
        return jsonify({'error': f'ä¸‹è½½å¤±è´¥: {str(e)}'}), 500

@app.route('/api/delete/<folder_name>', methods=['DELETE'])
def delete_folder(folder_name):
    """åˆ é™¤æŒ‡å®šæ–‡ä»¶å¤¹"""
    try:
        # å®‰å…¨æ£€æŸ¥ï¼šé˜²æ­¢è·¯å¾„éå†æ”»å‡»
        if '..' in folder_name or '/' in folder_name or '\\' in folder_name:
            return jsonify({'error': 'æ— æ•ˆçš„æ–‡ä»¶å¤¹åç§°'}), 400
        
        upload_dir = app.config['UPLOAD_FOLDER']
        folder_path = os.path.join(upload_dir, folder_name)
        
        if not os.path.exists(folder_path) or not os.path.isdir(folder_path):
            return jsonify({'error': 'æ–‡ä»¶å¤¹ä¸å­˜åœ¨'}), 404
        
        # åˆ é™¤æ–‡ä»¶å¤¹åŠå…¶å†…å®¹
        import shutil
        shutil.rmtree(folder_path)
        
        return jsonify({'message': f'æ–‡ä»¶å¤¹ {folder_name} åˆ é™¤æˆåŠŸ'})
    
    except Exception as e:
        return jsonify({'error': f'åˆ é™¤å¤±è´¥: {str(e)}'}), 500

# ç»§ç»­æœªå®Œæˆï¼šæ‰«ææ–‡ä»¶å¤¹ä¸­ç¼ºå¤±çš„MP3å¹¶ä»…å¤„ç†è¿™äº›MD
@app.route('/api/continue/<folder_name>', methods=['POST'])
def continue_folder(folder_name):
    try:
        # å®‰å…¨æ£€æŸ¥
        if '..' in folder_name or '/' in folder_name or '\\' in folder_name:
            return jsonify({'error': 'æ— æ•ˆçš„æ–‡ä»¶å¤¹åç§°'}), 400

        upload_dir = app.config['UPLOAD_FOLDER']
        folder_path = os.path.join(upload_dir, folder_name)
        if not os.path.exists(folder_path) or not os.path.isdir(folder_path):
            return jsonify({'error': 'æ–‡ä»¶å¤¹ä¸å­˜åœ¨'}), 404

        # è¯»å–å®¢æˆ·ç«¯é…ç½®
        api_servers_json = request.form.get('api_servers', '[]')
        concurrency = int(request.form.get('concurrency', 1))
        voice = request.form.get('voice', 'zh-CN-XiaoxiaoNeural')
        speed = float(request.form.get('speed', 1.0))

        try:
            api_servers = json.loads(api_servers_json)
        except json.JSONDecodeError:
            return jsonify({'error': 'APIæœåŠ¡å™¨é…ç½®æ ¼å¼é”™è¯¯'}), 400

        enabled_servers = [s for s in api_servers if s.get('enabled', True)]
        if not enabled_servers:
            return jsonify({'error': 'æ²¡æœ‰å¯ç”¨çš„APIæœåŠ¡å™¨'}), 400

        # æ‰¾å‡ºç¼ºå¤±çš„MP3å¯¹åº”çš„MD
        files = os.listdir(folder_path)
        md_files = [f for f in files if f.endswith('.md')]
        missing_md_files = []
        for md in md_files:
            mp3 = os.path.splitext(md)[0] + '.mp3'
            if mp3 not in files:
                missing_md_files.append(md)

        if not missing_md_files:
            return jsonify({'success': True, 'message': 'æ²¡æœ‰ç¼ºå¤±çš„ä»»åŠ¡ï¼Œå…¨éƒ¨å·²å®Œæˆ', 'batch_id': None, 'retry_files': 0})

        # åˆ›å»ºæ–°çš„batchä»¥å¤ç”¨ç°æœ‰è¿›åº¦ä¸è½®è¯¢æœºåˆ¶
        batch_id = str(uuid.uuid4())
        batch_status[batch_id] = {
            'total_files': len(missing_md_files),
            'completed_files': 0,
            'current_file': 0,
            'files': {},
            'server_statuses': {},
            'upload_dir': folder_path
        }

        # åˆå§‹åŒ–æ–‡ä»¶çŠ¶æ€å¹¶æ„é€ specific_filesåˆ—è¡¨ï¼ˆä½¿ç”¨batch_idå‰ç¼€çš„file_idï¼‰
        specific_files = []
        for md in missing_md_files:
            file_id = f"{batch_id}_{md}"
            batch_status[batch_id]['files'][file_id] = {
                'filename': md,
                'status': 'waiting',
                'progress': 0,
                'stage': 'ç­‰å¾…å¤„ç†'
            }
            specific_files.append(file_id)

        # å¯åŠ¨å¼‚æ­¥å¤„ç†ï¼Œä»…å¤„ç†ç¼ºå¤±é¡¹
        thread = threading.Thread(
            target=run_async_processing,
            args=(batch_id, folder_path, voice, speed, enabled_servers, concurrency, specific_files)
        )
        thread.daemon = True
        thread.start()

        return jsonify({
            'success': True,
            'message': f'å·²å¼€å§‹ç»§ç»­å¤„ç† {len(missing_md_files)} ä¸ªæœªå®Œæˆæ–‡ä»¶',
            'batch_id': batch_id,
            'retry_files': len(missing_md_files)
        })

    except Exception as e:
        print(f"ç»§ç»­æœªå®Œæˆå¤„ç†æ—¶å‡ºé”™: {str(e)}", file=sys.stderr)
        return jsonify({'error': f'ç»§ç»­å¤„ç†å¤±è´¥: {str(e)}'}), 500

# ç®€åŒ–çš„è´Ÿè½½å‡è¡¡å™¨é…ç½®
USE_SIMPLE_BALANCER = os.environ.get('USE_SIMPLE_BALANCER', 'true').lower() == 'true'

async def simple_load_balancer(batch_id, batch_upload_dir, voice, speed, api_servers, concurrency, specific_files=None):
    """è¶…ç®€å•çš„è´Ÿè½½å‡è¡¡å™¨ï¼šæ´¾å‘ä»»åŠ¡ â†’ 300ç§’è¶…æ—¶ç›‘æ§ â†’ è½®è¯¢æ£€æŸ¥"""
    if batch_id not in batch_status:
        return
    
    batch_info = batch_status[batch_id]
    
    # ç¡®å®šè¦å¤„ç†çš„æ–‡ä»¶
    if specific_files:
        files_to_process = specific_files
        print(f"ğŸ”„ é‡è¯•æ¨¡å¼: å¤„ç†æŒ‡å®šçš„ {len(files_to_process)} ä¸ªæ–‡ä»¶")
    else:
        # ä»batch_infoä¸­è·å–æ–‡ä»¶åˆ—è¡¨
        files_to_process = list(batch_info['files'].keys())
        print(f"ğŸ†• å…¨æ–°å¤„ç†: å¤„ç†æ‰€æœ‰ {len(files_to_process)} ä¸ªæ–‡ä»¶")
    
    if not files_to_process:
        print("ğŸ“ æ²¡æœ‰æ‰¾åˆ°è¦å¤„ç†çš„æ–‡ä»¶")
        return
    
    # å…¨å±€å¹¶å‘æ§åˆ¶ - æ ¸å¿ƒä¼˜åŒ–
    GLOBAL_CONCURRENCY_LIMIT = 8  # å…¨å±€å¹¶å‘é™åˆ¶ï¼Œé¿å…è§¦å‘APIé€Ÿç‡é™åˆ¶
    global_semaphore = asyncio.Semaphore(GLOBAL_CONCURRENCY_LIMIT)
    
    print(f"ğŸš€ å¯åŠ¨è¶…ç®€å•è´Ÿè½½å‡è¡¡å™¨:")
    print(f"  ğŸ“Š æ€»ä»»åŠ¡æ•°: {len(files_to_process)}")
    print(f"  ğŸ–¥ï¸ å¯ç”¨æœåŠ¡å™¨: {len(api_servers)}")
    print(f"  â° è¶…æ—¶æ—¶é—´: 300ç§’")
    print(f"  âš¡ å¹¶å‘åº¦: {concurrency}")
    print(f"  ğŸ¯ å…¨å±€å¹¶å‘é™åˆ¶: {GLOBAL_CONCURRENCY_LIMIT} (é˜²æ­¢APIé€Ÿç‡é™åˆ¶)")

    # æœåŠ¡å™¨çŠ¶æ€ç®¡ç†ï¼ˆå®¹é‡=æ¯å°æœåŠ¡å™¨å…è®¸çš„å¹¶å‘ï¼Œé»˜è®¤ä½¿ç”¨å…¨å±€concurrencyï¼Œå¯è¢«å•å°è¦†ç›–ï¼‰
    num_servers = len(api_servers)
    server_capacity = [int(api_servers[i].get('concurrency', concurrency)) for i in range(num_servers)]
    server_active = [0] * num_servers  # å½“å‰æ´»è·ƒä»»åŠ¡æ•°
    server_used_count = [0] * num_servers  # è¢«æ´¾å‘è¿‡æ¬¡æ•°
    server_last_used = [0.0] * num_servers  # æœ€åä¸€æ¬¡æ´¾å‘æ—¶é—´
    server_completed = [0] * num_servers
    server_failed = [0] * num_servers
    server_consecutive_failures = [0] * num_servers  # è¿ç»­å¤±è´¥æ¬¡æ•°ï¼ˆç”¨äºç†”æ–­ï¼‰
    last_dispatch_index = 0  # è½®è¯¢æŒ‡é’ˆï¼Œæ‰“æ•£åŒç­‰æ¡ä»¶æœåŠ¡å™¨çš„é€‰æ‹©

    # åˆå§‹åŒ–WebUIçš„æœåŠ¡å™¨çŠ¶æ€ï¼ˆç®€åŒ–æ¨¡å¼ä¹Ÿä¸ŠæŠ¥ï¼‰
    for i in range(num_servers):
        batch_info['server_statuses'][i] = {
            'name': api_servers[i]['name'],
            'status': 'idle',
            'load': 0,
            'max_load': server_capacity[i],
            'completed_tasks': 0,
            'total_time': 0
        }

    # ä»»åŠ¡é˜Ÿåˆ—å’ŒçŠ¶æ€
    task_queue = asyncio.Queue()
    retry_queue = asyncio.Queue()
    active_tasks = set()
    completed_tasks = 0
    total_tasks = len(files_to_process)
    task_retries = {}
    max_retries = 3
    server_cooldown_until = [0.0] * num_servers  # æ•…éšœå†·å´æˆªæ­¢æ—¶é—´æˆ³

    # åˆå§‹åŒ–ä»»åŠ¡é˜Ÿåˆ—
    for file_id in files_to_process:
        await task_queue.put(file_id)

    def find_available_server():
        """æ‰¾å¯ç”¨æœåŠ¡å™¨ï¼šæœªç”¨è¿‡ä¼˜å…ˆ â†’ æœ€å°è´Ÿè½½ â†’ å¤±è´¥ç‡ä½ â†’ æœ€ä¹…æœªç”¨ï¼ˆå«ç†”æ–­æ£€æŸ¥ï¼‰"""
        now_ts = time.time()
        candidates = [
            i for i in range(num_servers)
            if server_active[i] < server_capacity[i] and now_ts >= server_cooldown_until[i]
        ]
        if not candidates:
            return None

        # è½®è¯¢æ‰“æ•£ï¼Œé¿å…æ€»æ˜¯é€‰åˆ°ç›¸åŒç´¢å¼•
        rotated = candidates[last_dispatch_index % len(candidates):] + candidates[:last_dispatch_index % len(candidates)]

        # è®¡ç®—æ’åºé”®
        def sort_key(i: int):
            total = server_completed[i] + server_failed[i]
            fail_rate = (server_failed[i] / total) if total > 0 else 0.0
            return (
                0 if server_used_count[i] == 0 else 1,  # æœªç”¨è¿‡ä¼˜å…ˆ
                server_active[i],                          # å½“å‰è´Ÿè½½è¶Šå°è¶Šä¼˜å…ˆ
                server_used_count[i],                      # ä½¿ç”¨æ¬¡æ•°æ›´å°‘æ›´ä¼˜å…ˆï¼ˆä¿ƒå‡è¡¡ï¼‰
                fail_rate,                                  # å¤±è´¥ç‡æ›´ä½ä¼˜å…ˆ
                server_last_used[i]                         # æœ€ä¹…æœªç”¨ä¼˜å…ˆï¼ˆæ—¶é—´æ›´æ—©ï¼‰
            )

        return min(rotated, key=sort_key)
    
    async def process_task(file_id, server_id, pre_reserved: bool = False):
        """å¤„ç†å•ä¸ªä»»åŠ¡ï¼šæ´¾å‘ â†’ 300ç§’è¶…æ—¶ç›‘æ§ â†’ è½®è¯¢æ£€æŸ¥"""
        nonlocal completed_tasks
        
        # å…¨å±€å¹¶å‘æ§åˆ¶ - è·å–ä¿¡å·é‡è®¸å¯
        async with global_semaphore:
            server_name = api_servers[server_id]['name']
            print(f"ğŸ“¤ æ´¾å‘ä»»åŠ¡: {file_id} â†’ {server_name} (å…¨å±€å¹¶å‘: {GLOBAL_CONCURRENCY_LIMIT - global_semaphore._value}/{GLOBAL_CONCURRENCY_LIMIT})")
            
            # æ›´æ–°æ–‡ä»¶çŠ¶æ€
            if file_id in batch_info['files']:
                file_info = batch_info['files'][file_id]
                file_info['status'] = 'processing'
                file_info['progress'] = 10
                file_info['stage'] = f'ğŸµ æ­£åœ¨è½¬æ¢ (æœåŠ¡å™¨: {server_name})...'
            
            # é¢„å å®¹é‡ï¼šè‹¥æœªåœ¨æ´¾å‘ç¯èŠ‚é¢„å ï¼Œè¿™é‡Œè¡¥å……å ç”¨
            if not pre_reserved:
                server_active[server_id] += 1
            server_used_count[server_id] += 1
            server_last_used[server_id] = time.time()
            active_tasks.add(file_id)

            # æ›´æ–°WebUIæœåŠ¡å™¨çŠ¶æ€
            batch_info['server_statuses'][server_id]['load'] = server_active[server_id]
            batch_info['server_statuses'][server_id]['status'] = (
                'full' if server_active[server_id] >= server_capacity[server_id] else 'busy'
            )
            
            start_time = time.time()
            success = False
            
            try:
                # è¯»å–æ–‡ä»¶å†…å®¹
                filename = batch_info['files'][file_id]['filename']
                input_path = os.path.join(batch_upload_dir, filename)
                output_path = os.path.join(batch_upload_dir, filename.replace('.md', '.mp3'))
                
                if os.path.exists(input_path):
                    with open(input_path, 'r', encoding='utf-8') as f:
                        text = f.read()
                    
                    # åˆ›å»ºHTTPä¼šè¯å¹¶è°ƒç”¨TTS
                    async with aiohttp.ClientSession() as session:
                        success, status_code, error_detail = await async_text_to_speech(
                            session, text, output_path, voice, speed,
                            api_servers[server_id]['url'], 
                            api_servers[server_id].get('apiKey', api_servers[server_id].get('api_key', '')),
                            timeout_seconds=300  # æ˜ç¡®ä¼ é€’è¶…æ—¶å‚æ•°
                        )
                    
                    processing_time = time.time() - start_time
                    
                    if success:
                        server_completed[server_id] += 1
                        completed_tasks += 1
                        batch_info['completed_files'] = completed_tasks
                        batch_info['current_file'] = completed_tasks
                        
                        # æ›´æ–°æ–‡ä»¶çŠ¶æ€
                        if file_id in batch_info['files']:
                            file_info = batch_info['files'][file_id]
                            file_info['status'] = 'completed'
                            file_info['progress'] = 100
                            file_info['stage'] = 'âœ… è½¬æ¢å®Œæˆ'
                        
                        # æˆåŠŸåæ¸…é™¤è¯¥æœåŠ¡å™¨çš„å†·å´ã€é‡è¯•è®¡æ•°å’Œè¿ç»­å¤±è´¥è®¡æ•°
                        server_cooldown_until[server_id] = 0.0
                        server_consecutive_failures[server_id] = 0  # é‡ç½®è¿ç»­å¤±è´¥è®¡æ•°
                        if file_id in task_retries:
                            task_retries[file_id] = 0
                        
                        print(f"âœ… ä»»åŠ¡å®Œæˆ: {filename} (æœåŠ¡å™¨: {server_name}, è€—æ—¶: {processing_time:.2f}ç§’)")
                    else:
                        server_failed[server_id] += 1
                        server_consecutive_failures[server_id] += 1  # å¢åŠ è¿ç»­å¤±è´¥è®¡æ•°
                        if status_code is not None:
                            print(f"âŒ ä»»åŠ¡å¤±è´¥: {filename} (æœåŠ¡å™¨: {server_name}, çŠ¶æ€ç : {status_code}, è€—æ—¶: {processing_time:.2f}ç§’)")
                        else:
                            print(f"âŒ ä»»åŠ¡å¤±è´¥: {filename} (æœåŠ¡å™¨: {server_name}, è€—æ—¶: {processing_time:.2f}ç§’)")
                        
                        # æœåŠ¡å™¨ç†”æ–­ï¼šè¿ç»­å¤±è´¥3æ¬¡åç†”æ–­60ç§’
                        if server_consecutive_failures[server_id] >= 3:
                            server_cooldown_until[server_id] = time.time() + 60.0
                            print(f"ğŸ”¥ æœåŠ¡å™¨ {server_name} ç†”æ–­60ç§’ (è¿ç»­å¤±è´¥{server_consecutive_failures[server_id]}æ¬¡)")
                        else:
                            # çŸ­æš‚å†·å´è¯¥æœåŠ¡å™¨ï¼Œé¿å…æŒç»­æ´¾å‘åˆ°ä¸å¥åº·èŠ‚ç‚¹
                            server_cooldown_until[server_id] = time.time() + 10.0
                        
                        # æ›´æ–°æ–‡ä»¶çŠ¶æ€
                        if file_id in batch_info['files']:
                            file_info = batch_info['files'][file_id]
                            file_info['status'] = 'failed'
                            file_info['progress'] = 100
                            file_info['stage'] = f'âŒ è½¬æ¢å¤±è´¥ (æœåŠ¡å™¨: {server_name})'
                        
                        # æ™ºèƒ½é‡è¯•ï¼šæŒ‡æ•°é€€é¿
                        current_retry = task_retries.get(file_id, 0)
                        if current_retry < max_retries:
                            task_retries[file_id] = current_retry + 1
                            # è®¡ç®—é€€é¿æ—¶é—´ï¼š2^retry + éšæœºæŠ–åŠ¨
                            backoff_time = (2 ** current_retry) + random.uniform(0, 1)
                            print(f"ğŸ”„ ä»»åŠ¡å°†é‡è¯• (ç¬¬ {current_retry+1} æ¬¡)ï¼Œå°†åœ¨ {backoff_time:.2f} ç§’åæ‰§è¡Œ...")
                            # å»¶è¿Ÿé‡è¯•
                            await asyncio.sleep(backoff_time)
                            await retry_queue.put(file_id)
                else:
                    print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {filename}")
                    current_retry = task_retries.get(file_id, 0)
                    if current_retry < max_retries:
                        task_retries[file_id] = current_retry + 1
                        await retry_queue.put(file_id)
                    
            except asyncio.TimeoutError:
                processing_time = time.time() - start_time
                server_failed[server_id] += 1
                server_consecutive_failures[server_id] += 1  # å¢åŠ è¿ç»­å¤±è´¥è®¡æ•°
                print(f"â° ä»»åŠ¡è¶…æ—¶: {filename} (æœåŠ¡å™¨: {server_name}, è€—æ—¶: {processing_time:.2f}ç§’)")
                
                # æœåŠ¡å™¨ç†”æ–­ï¼šè¿ç»­å¤±è´¥3æ¬¡åç†”æ–­60ç§’
                if server_consecutive_failures[server_id] >= 3:
                    server_cooldown_until[server_id] = time.time() + 60.0
                    print(f"ğŸ”¥ æœåŠ¡å™¨ {server_name} ç†”æ–­60ç§’ (è¿ç»­å¤±è´¥{server_consecutive_failures[server_id]}æ¬¡)")
                else:
                    server_cooldown_until[server_id] = time.time() + 10.0
                
                # æ›´æ–°æ–‡ä»¶çŠ¶æ€
                if file_id in batch_info['files']:
                    file_info = batch_info['files'][file_id]
                    file_info['status'] = 'failed'
                    file_info['progress'] = 100
                    file_info['stage'] = f'â° è½¬æ¢è¶…æ—¶ (æœåŠ¡å™¨: {server_name})'
                
                # æ™ºèƒ½é‡è¯•ï¼šæŒ‡æ•°é€€é¿
                current_retry = task_retries.get(file_id, 0)
                if current_retry < max_retries:
                    task_retries[file_id] = current_retry + 1
                    backoff_time = (2 ** current_retry) + random.uniform(0, 1)
                    print(f"ğŸ”„ ä»»åŠ¡å°†é‡è¯• (ç¬¬ {current_retry+1} æ¬¡)ï¼Œå°†åœ¨ {backoff_time:.2f} ç§’åæ‰§è¡Œ...")
                    await asyncio.sleep(backoff_time)
                    await retry_queue.put(file_id)
                
            except Exception as e:
                processing_time = time.time() - start_time
                server_failed[server_id] += 1
                server_consecutive_failures[server_id] += 1  # å¢åŠ è¿ç»­å¤±è´¥è®¡æ•°
                print(f"âŒ ä»»åŠ¡å¼‚å¸¸: {filename} (æœåŠ¡å™¨: {server_name}, è€—æ—¶: {processing_time:.2f}ç§’, é”™è¯¯: {e})")
                
                # æœåŠ¡å™¨ç†”æ–­ï¼šè¿ç»­å¤±è´¥3æ¬¡åç†”æ–­60ç§’
                if server_consecutive_failures[server_id] >= 3:
                    server_cooldown_until[server_id] = time.time() + 60.0
                    print(f"ğŸ”¥ æœåŠ¡å™¨ {server_name} ç†”æ–­60ç§’ (è¿ç»­å¤±è´¥{server_consecutive_failures[server_id]}æ¬¡)")
                else:
                    server_cooldown_until[server_id] = time.time() + 10.0
                
                # æ›´æ–°æ–‡ä»¶çŠ¶æ€
                if file_id in batch_info['files']:
                    file_info = batch_info['files'][file_id]
                    file_info['status'] = 'failed'
                    file_info['progress'] = 100
                    file_info['stage'] = f'ğŸ’¥ å¤„ç†å¼‚å¸¸: {str(e)}'
                
                # æ™ºèƒ½é‡è¯•ï¼šæŒ‡æ•°é€€é¿
                current_retry = task_retries.get(file_id, 0)
                if current_retry < max_retries:
                    task_retries[file_id] = current_retry + 1
                    backoff_time = (2 ** current_retry) + random.uniform(0, 1)
                    print(f"ğŸ”„ ä»»åŠ¡å°†é‡è¯• (ç¬¬ {current_retry+1} æ¬¡)ï¼Œå°†åœ¨ {backoff_time:.2f} ç§’åæ‰§è¡Œ...")
                    await asyncio.sleep(backoff_time)
                    await retry_queue.put(file_id)
            
            finally:
                # é‡Šæ”¾æœåŠ¡å™¨ï¼ˆä¸æ´¾å‘æ—¶çš„é¢„å å¯¹åº”ï¼Œä»…å‡ä¸€æ¬¡ï¼‰
                server_active[server_id] = max(0, server_active[server_id] - 1)
                active_tasks.discard(file_id)
                # æ›´æ–°WebUIæœåŠ¡å™¨çŠ¶æ€
                batch_info['server_statuses'][server_id]['load'] = server_active[server_id]
                batch_info['server_statuses'][server_id]['completed_tasks'] = server_completed[server_id]
                if server_active[server_id] == 0:
                    batch_info['server_statuses'][server_id]['status'] = 'idle'
                elif server_active[server_id] >= server_capacity[server_id]:
                    batch_info['server_statuses'][server_id]['status'] = 'full'
                else:
                    batch_info['server_statuses'][server_id]['status'] = 'busy'
                print(f"ğŸ”„ æœåŠ¡å™¨ {server_name} å·²é‡Šæ”¾ (è´Ÿè½½: {server_active[server_id]}/{server_capacity[server_id]})")
    
    # ä¸»å¤„ç†å¾ªç¯
    print(f"ğŸ¯ å¼€å§‹ä»»åŠ¡åˆ†é…å¾ªç¯...")
    
    while completed_tasks < total_tasks:
        assigned_any = False
        # å°½å¯èƒ½åœ¨åŒä¸€å¾ªç¯å†…å¡«æ»¡æ‰€æœ‰å¯ç”¨å®¹é‡
        while True:
            server_id = find_available_server()
            if server_id is None:
                break

            # é€‰æ‹©ä»»åŠ¡ï¼šé‡è¯•ä¼˜å…ˆ
            file_id = None
            if not retry_queue.empty():
                try:
                    file_id = retry_queue.get_nowait()
                    print(f"ğŸ”„ é‡è¯•ä»»åŠ¡: {file_id}")
                except asyncio.QueueEmpty:
                    pass
            if file_id is None and not task_queue.empty():
                try:
                    file_id = task_queue.get_nowait()
                except asyncio.QueueEmpty:
                    pass
            if file_id is None:
                break

            # æŒ‡æ•°é€€é¿ï¼ˆé‡è¯•ä»»åŠ¡ï¼‰
            retries = task_retries.get(file_id, 0)
            if retries > 0:
                backoff = min(60.0, (2 ** (retries - 1)) + random.uniform(0, 0.5))
                await asyncio.sleep(backoff)

            # é¢„å å®¹é‡
            server_active[server_id] += 1
            batch_info['server_statuses'][server_id]['load'] = server_active[server_id]
            batch_info['server_statuses'][server_id]['status'] = (
                'full' if server_active[server_id] >= server_capacity[server_id] else 'busy'
            )
            asyncio.create_task(process_task(file_id, server_id, pre_reserved=True))
            last_dispatch_index = (last_dispatch_index + 1) % max(1, len([i for i in range(num_servers) if server_capacity[i] > 0]))
            assigned_any = True

        if not assigned_any:
            # æ— å¯æ´¾å‘åˆ™ç­‰å¾…
            active_count = sum(server_active)
            total_capacity = sum(server_capacity)
            idle_count = total_capacity - active_count
            
            if active_count >= total_capacity:
                print(f"â³ æ‰€æœ‰æœåŠ¡å™¨å®¹é‡å·²æ»¡ ({active_count}/{total_capacity})ï¼Œç­‰å¾…ä»»åŠ¡å®Œæˆ...")
                await asyncio.sleep(1)
            elif task_queue.empty() and retry_queue.empty():
                if active_count > 0:
                    print(f"ğŸ“­ é˜Ÿåˆ—ä¸ºç©ºï¼Œç­‰å¾… {active_count} ä¸ªæ­£åœ¨è¿è¡Œçš„ä»»åŠ¡å®Œæˆ...")
                else:
                    print(f"âœ… æ‰€æœ‰ä»»åŠ¡å¤„ç†å®Œæ¯•ï¼Œç³»ç»Ÿç©ºé—²ã€‚")
                await asyncio.sleep(1)
            else:
                print(f"ğŸ”„ éƒ¨åˆ†æœåŠ¡å™¨ç©ºé—² ({idle_count}/{total_capacity})ï¼Œç»§ç»­åˆ†é…ä»»åŠ¡...")
                await asyncio.sleep(0.2)
    
    # ç­‰å¾…æ‰€æœ‰æ´»è·ƒä»»åŠ¡å®Œæˆ
    while active_tasks:
        print(f"â³ ç­‰å¾… {len(active_tasks)} ä¸ªæ´»è·ƒä»»åŠ¡å®Œæˆ...")
        await asyncio.sleep(1)
    
    print(f"ğŸ‰ è¶…ç®€å•è´Ÿè½½å‡è¡¡å™¨å¤„ç†å®Œæˆ")
    print(f"ğŸ“Š æœ€ç»ˆç»Ÿè®¡: å®Œæˆ {completed_tasks}/{total_tasks} ä¸ªä»»åŠ¡")
    
    # è¾“å‡ºè¯¦ç»†çš„æœåŠ¡å™¨ç»Ÿè®¡
    print(f"ğŸ“Š æœåŠ¡å™¨æ€§èƒ½ç»Ÿè®¡:")
    for i in range(len(api_servers)):
        server_name = api_servers[i]['name']
        total_used = server_completed[i] + server_failed[i]
        if total_used > 0:
            success_rate = (server_completed[i] / total_used) * 100
            print(f"  ğŸ–¥ï¸ {server_name}:")
            print(f"    âœ… å®Œæˆä»»åŠ¡: {server_completed[i]} ä¸ª")
            print(f"    âŒ å¤±è´¥ä»»åŠ¡: {server_failed[i]} ä¸ª")
            print(f"    ğŸ“ˆ æˆåŠŸç‡: {success_rate:.1f}%")
            print(f"    ğŸ”„ æ€»ä½¿ç”¨: {total_used} æ¬¡")

if __name__ == '__main__':
    # æ”¯æŒDockeréƒ¨ç½²ï¼Œç›‘å¬æ‰€æœ‰æ¥å£
    import os
    host = os.environ.get('FLASK_HOST', '0.0.0.0')
    port = int(os.environ.get('FLASK_PORT', 5055))
    debug = os.environ.get('FLASK_ENV', 'development') == 'development'
    
    print(f"ğŸš€ å¯åŠ¨TTSæ‰¹é‡è½¬æ¢æœåŠ¡...")
    print(f"ğŸ“ ç›‘å¬åœ°å€: {host}:{port}")
    print(f"ğŸ”§ è°ƒè¯•æ¨¡å¼: {debug}")
    if USE_SIMPLE_BALANCER:
        print(f"âš¡ ä½¿ç”¨åŠ¨æ€å·¥ä½œèŠ‚ç‚¹è´Ÿè½½å‡è¡¡å™¨ (V4)")
    else:
        print(f"âš¡ ä½¿ç”¨å¤æ‚è´Ÿè½½å‡è¡¡å™¨ï¼ˆæ—§ç‰ˆè·¯å¾„ï¼‰")
    
    app.run(host=host, port=port, debug=debug)
